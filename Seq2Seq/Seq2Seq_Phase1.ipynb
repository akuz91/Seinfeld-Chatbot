{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq_Phase1.ipynb","provenance":[],"collapsed_sections":["9KDkP9g3nFDu","-IA8ME8Kni-F","wm8anfhTWURY","mfJ0kRarrLMT","Sg2MR3WfGAZx","qc-8xDpNHHDZ","QSTGexYN-CRV","SwzcG7cj-G8i","P0wN_CWvUMtu","dPmiacjoRw3i","f0OqI0OzSMD9"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7ArAVjoGKEKN"},"source":["# **Seq2Seq (GRU)**\n","\n","Complex version is trained on dataset that follows an entire conversation between two characters where each new line is the input and the following line is the output (label). The persona of the label's speaker is stored with the input. When a new character is introduced into the scene, a new conversation is declared."]},{"cell_type":"markdown","metadata":{"id":"9KDkP9g3nFDu"},"source":["## **Setup**"]},{"cell_type":"code","metadata":{"id":"gbDELiPXnJda","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606706959259,"user_tz":300,"elapsed":17397,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"eaa37edd-8867-4d76-a552-d99f3f74ac60"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ERYRsXLYnNRr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606706959533,"user_tz":300,"elapsed":17663,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"5d9b1a59-b070-49bf-8316-fee2eaefcd43"},"source":["cd /content/drive/My Drive/1011: Term Project/Collab Notebooks"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1ZU9nsIwPF3oR15kSIr3Os3Q6e-CKHxOT/1011: Term Project/Collab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MpQi9K_2nZnY","executionInfo":{"status":"ok","timestamp":1606706963636,"user_tz":300,"elapsed":21763,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["# Import required packages\n","import os\n","import json\n","import numpy as np\n","from collections import defaultdict\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence #added\n","from torch.utils.data import Dataset, DataLoader #added\n","import torch.optim as optim #added\n","from torch.nn import Embedding #added\n","from torch.nn import Linear #added\n","\n","from tqdm import tqdm #added\n","import tensorflow as tf #added\n","import matplotlib.pyplot as plt #added\n","import pandas as pd #added\n","import re #added\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmc29-ZbjlK1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606706970631,"user_tz":300,"elapsed":28752,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"7b443672-fa12-493d-dc26-2cc1b9efa68e"},"source":["pip install transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 15.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 48.5MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 64.6MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 41.9MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4e747a6e921c2eed9e34be984f71313a0b48b9902ea6a388988222b955838aa8\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FCCkOQvvjouK","executionInfo":{"status":"ok","timestamp":1606706972978,"user_tz":300,"elapsed":31094,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["!cd \"/content/drive/MyDrive/1011: Term Project/Collab Notebooks/Dialogue/\"\n","import sys\n","sys.path.append('/content/drive/MyDrive/1011: Term Project/Collab Notebooks/Dialogue/')\n","from sienfield_utils import clean_df"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-IA8ME8Kni-F"},"source":["## **Load Seinfeld Data**"]},{"cell_type":"code","metadata":{"id":"iNsyt1pGpsUG","executionInfo":{"status":"ok","timestamp":1606707345414,"user_tz":300,"elapsed":1192,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["#import Seinfeld scripts\n","df=pd.read_csv('/content/drive/My Drive/1011: Term Project/Collab Notebooks/data/scripts.csv')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"afrvJQLAsovX","executionInfo":{"status":"ok","timestamp":1606707345416,"user_tz":300,"elapsed":779,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["#import train personas of main characters\n","train_personas = {\"JERRY\": [\"your persona: i am a comedian. \\nyour persona: i am a compulsive neat freak. \\nyour persona: i like to read comic books. \\nyour persona: i break up with girls for superficial reasons.\"],\n","           \"GEORGE\": [\"your persona: i am very neurotic and always afraid that nobody likes me. \\nyour persona: i am selfish and greedy. \\nyour persona: i have low self-esteem. \\nyour persona: i am cheap.\"],\n","            \"ELAINE\": [\"your persona: i am Jerry's ex-girlfriend. \\nyour persona: i am the best-educated of my group of friends. \\nyour persona: i am cynical and acid-tongued. \\nyour persona: i work for a publishing company.\"],\n","            \"KRAMER\": [\"your persona: i am Jerry's wacky neighbor. \\nyour persona: i don't have a job. \\nyour persona: i am caring and friendly. \\nyour persona: i am extremely honest and lack tact.\"]\n","          }\n","#import test personas of main characters\n","test_personas = {\"JERRY\": [\"your persona: i make a living telling jokes. \\nyour persona: i like to watch sports. \\nyour persona: i am a fan of cartoons. \\nyour persona: women find me quirky and charming.\"],\n","                  \"GEORGE\": [\"your persona: i am vulnerable and slightly neurotic. \\nyour persona: i am short, stocky, and slow-witted. \\nyour persona: i am dishonest. \\nyour persona: i have eccentric behavior.\"],\n","                  \"ELAINE\": [\"your persona: i am intelligent. \\nyour persona: i am funny. \\nyour persona: i am assertive and confident. \\nyour persona: i am edgy and superficial.\"],\n","                  \"KRAMER\": [\"your persona: i have eccentric behavior. \\nyour persona: i am unemployed. \\nyour persona: i always tell the truth. \\nyour persona: i have interesting ideas.\"]\n","                  }"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wm8anfhTWURY"},"source":["# **Clean Seinfeld Data**"]},{"cell_type":"code","metadata":{"id":"A-RG5Br4WWeJ","executionInfo":{"status":"ok","timestamp":1606707349919,"user_tz":300,"elapsed":1923,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["#clean data with util function\n","df = clean_df(df)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-hZMuCbOItT","executionInfo":{"status":"ok","timestamp":1606707353813,"user_tz":300,"elapsed":624,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"286aefcc-b0ed-4704-c87e-6d0864035726"},"source":["len(df)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["54606"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"9UihL3V7WY--","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1606707349920,"user_tz":300,"elapsed":1258,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"2fb9e540-219c-4d54-f58d-8f977ae0ff76"},"source":["df.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Character</th>\n","      <th>Dialogue</th>\n","      <th>EpisodeNo</th>\n","      <th>SEID</th>\n","      <th>Season</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>JERRY</td>\n","      <td>do you know what this is all about ? do you kn...</td>\n","      <td>1.0</td>\n","      <td>S01E01</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>JERRY</td>\n","      <td>see , to me , that button is in the worst poss...</td>\n","      <td>1.0</td>\n","      <td>S01E01</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>GEORGE</td>\n","      <td>are you through ?</td>\n","      <td>1.0</td>\n","      <td>S01E01</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>JERRY</td>\n","      <td>you do of course try on , when you buy ?</td>\n","      <td>1.0</td>\n","      <td>S01E01</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>GEORGE</td>\n","      <td>yes , it was purple , i liked it , i dont actu...</td>\n","      <td>1.0</td>\n","      <td>S01E01</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0 Character  ...    SEID  Season\n","0           0     JERRY  ...  S01E01     1.0\n","1           1     JERRY  ...  S01E01     1.0\n","2           2    GEORGE  ...  S01E01     1.0\n","3           3     JERRY  ...  S01E01     1.0\n","4           4    GEORGE  ...  S01E01     1.0\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"SmcLJD8hWmPz","executionInfo":{"status":"ok","timestamp":1606708074827,"user_tz":300,"elapsed":589,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["id = list(df['Dialogue'].index)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"SH6vD_nUWhsw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606708075821,"user_tz":300,"elapsed":1321,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"a13fd79e-692a-4381-eebc-c3aa0ee19a8b"},"source":["#check for open brackets\n","for i in id:\n","  if '(' in df['Dialogue'][i] and ')' not in  df['Dialogue'][i]:\n","    print(i, df['Dialogue'][i])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["3067 hey , you see that sign right there ? ( points to a sign saying \" not responsible for valuables \"\n","6993 ( to himself uh oh . my organs are playing chess again .\n","7111 ( waving eva .\n","7128 ( patting his head i ' m a comedian .\n","28553 ( tilts her head down , looking over her glasses in amazement of\n","30966 \" kom pau ( sp ? \"\n","31163 \" yes it is . well lets see what i have today . darn it it ' s ham & cheese again and she forgot the fancy mustard . i told her i like that fancy mustard . you could put that fancy mustard on a shoe and it would taste pretty good to me . oh ! she made it up with a cupcake though . hey look at this . you know i got a new system for eating these things . i used to peel off the chocolate now i turn them upside down , i eat the cake first and save the frosting for the end . ( george stops listening and it ' s almost like its own dessert ;\n","31206 yeah . ( reaches for the purse and finds a piece of paper . he looks annoyed .\n","39830 now , what are you thinkin ' ? you think that i ' m not able to wear jeans anymore ? is that what you ' re sayin ' ? because if that ' s what you ' re sayin ' , jerry , i ' ll go and i ' ll buy some jeans . i swear to god i will ! ( jerry ' s showing off a skeptical face .\n","43280 oh , right ! right ! hey , hey ; i love the floors in here . it ' s like a gymnasium in here ! try and guard me ! ( dribbles an imaginary ball\n","49780 spite never sleeps ( doing a little dance as she says it\n","52191 ( overhearing the other two elaine , elaine - -\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m2q2XY1-Wkkq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606708076465,"user_tz":300,"elapsed":993,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"07b61104-cddf-4e42-ead8-92a93a8e5838"},"source":["#check for open square brackets\n","for i in id:\n","  if '[' in df['Dialogue'][i] and ']' not in  df['Dialogue'][i]:\n","    print(i, df['Dialogue'][i])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["28739 well , one cannot help [ but wonder what brings you into a crummy little coffee shop like this .\n","45963 jerry , i ' m trapped under my desk . steinbrenner is in the room . you got to help [ me .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3IUXZ5CaXQoa","executionInfo":{"status":"ok","timestamp":1606708076467,"user_tz":300,"elapsed":743,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["#manual adjustments\n","df['Dialogue'][3067] = 'hey , you see that sign right there ? '\n","df['Dialogue'][6993] = 'uh oh. my organs are playing chess again.'\n","df['Dialogue'][7128] = \"i'm a comedian.\"\n","df['Dialogue'][30966]  = 'kom pau'\n","df['Dialogue'][31163] = \"yes it is . well lets see what I have today . darn it it ' s ham & cheese again and she forgot the fancy mustard . i told her i like that fancy mustard . you could put that fancy mustard on a shoe and it would taste pretty good to me . oh ! she made it up with a cupcake though . hey look at this . you know i got a new system for eating these things . i used to peel off the chocolate now i turn them upside down , i eat the cake first and save the frosting for the end .\"\n","df['Dialogue'][31206] = 'yeah .'\n","df['Dialogue'][39830] = \"now , what are you thinkin ' ? you think that i ' m not able to wear jeans anymore ? is that what you ' re sayin ' ? because if that ' s what you ' re sayin ' , jerry , i ' ll go and i ' ll buy some jeans . i swear to god i will ! \"\n","df['Dialogue'][43280] = \"oh , right ! right ! hey , hey ; i love the floors in here . it ' s like a gymnasium in here ! try and guard me !\"\n","df['Dialogue'][49780] = \"spite never sleeps \"\n","df['Dialogue'][52191] = 'elaine, elaine'\n","df['Dialogue'][28739] = 'well, one cannot help but wonder what brings you into a crummy little coffee shop like this.'\n","df['Dialogue'][45963] = \"jerry , i ' m trapped under my desk . steinbrenner is in the room . you got to help me .\""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"01BTzs6OaOcn","executionInfo":{"status":"ok","timestamp":1606708077135,"user_tz":300,"elapsed":370,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["#need to remove action only lines: 7111, 28553\n","df = df.drop(index=7111)\n","df = df.drop(index=28553)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYbf3kDtaxSp","executionInfo":{"status":"ok","timestamp":1606708077502,"user_tz":300,"elapsed":480,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["#reset indices after dropping rows\n","df = df.reset_index(drop=True)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mfJ0kRarrLMT"},"source":["# **Creating and Splitting Seinfeld Datasets**"]},{"cell_type":"code","metadata":{"id":"rWIkE1QMnoZc","executionInfo":{"status":"ok","timestamp":1606708079756,"user_tz":300,"elapsed":785,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["def create_datasets(df, personas, episodes):\n","#keeps conversation history until new character enters conversation; then new convo starts\n","\n","  data ={}\n","  data=[]\n","\n","  #main characters building labels for\n","  chars = ['JERRY','GEORGE','ELAINE','KRAMER']\n","\n","  for ep in episodes:\n","    c = 0 #keep track of ind in data (+1 every time something is added to data)\n","    check = False #keeps track if prev speaker was non main so we don't append their line in the convo history\n","\n","    #get df of transcripts just from specific episode\n","    episode = df[df['SEID']==str(ep)]\n","\n","    #remove Jerry's monologue\n","    episode = episode[1:]\n","    episode = episode.reset_index(drop=True)\n","\n","    #initialize first two speakers\n","    curr_char = [[episode['Character'][0],episode['Character'][1]]]\n","\n","    for i in range(len(episode)-2):\n","\n","      #if curr char is same as next, SKIP\n","      if curr_char[i][0] == curr_char[i][1]:\n","\n","        #move to next pair of speakers\n","        curr_char.append([episode['Character'][i+1],episode['Character'][i+2]])\n","        check=False\n","      \n","      #if next char (label) is not a main char, SKIP and set bool to True so we remember not to append this speaker's lines\n","      elif  curr_char[i][1] not in chars:\n","        check = True\n","        curr_char.append([episode['Character'][i+1],episode['Character'][i+2]])\n","\n","      #if next char not in curr conversation (not in list of curr chars), make new convo\n","      elif i>=1 and curr_char[i][1] not in curr_char[i-1]:\n","\n","        #there is no previous chat history of the two chars, add new info\n","\n","        #check if speaker 1 is a main\n","        #if they aren't, don't look up previous persona in text (because the previous line was not saved - non-main char would have been label)\n","        if curr_char[i][0] not in chars:\n","          data.append({'text':personas[episode['Character'][i+1]][0]+'\\n'+episode['Dialogue'][i],'labels':[episode['Dialogue'][i+1]], 'char':episode['Character'][i+1]})\n","        \n","        #otherwise, remove previous persona from text\n","        else:\n","          data.append({'text':personas[episode['Character'][i+1]][0]+'\\n'+episode['Dialogue'][i].replace(personas[episode['Character'][i]][0]+'\\n',''),'labels':[episode['Dialogue'][i+1]], 'char':episode['Character'][i+1]})\n","\n","        #move to next pair of speakers\n","        curr_char.append([episode['Character'][i+1],episode['Character'][i+2]])\n","        c+=1\n","        check=False\n","\n","      else:\n","        #if first element in data, just add new info\n","\n","        #check bool if previous char is not main; if T: start new convo\n","        if check == True or i < 1:\n","          data.append({'text':personas[episode['Character'][i+1]][0]+'\\n'+episode['Dialogue'][i],'labels':[episode['Dialogue'][i+1]], 'char': episode['Character'][i+1]})\n","          c+=1\n","          check = False\n","\n","        #if there is previous chat history, concat from previous and add new info\n","        else:\n","          data.append({'text':personas[episode['Character'][i+1]][0]+'\\n'+data[c-1]['text'].replace(personas[episode['Character'][i]][0]+'\\n','')+'\\n'+episode['Dialogue'][i],'labels':[episode['Dialogue'][i+1]], 'char': episode['Character'][i+1]})\n","          c+=1\n","          check=False\n","\n","        #move to next pair of speakers\n","        curr_char.append([episode['Character'][i+1],episode['Character'][i+2]])\n","\n","  return data"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9Qu-c26zA8K","executionInfo":{"status":"ok","timestamp":1606708082202,"user_tz":300,"elapsed":666,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["#get list of episodes\n","episodes_list = df['SEID'].unique().tolist()\n","\n","#shuffle episodes list\n","import random\n","random.seed(2020)\n","random.shuffle(episodes_list)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"sx3UWaQCzFa4","executionInfo":{"status":"ok","timestamp":1606708102168,"user_tz":300,"elapsed":549,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}}},"source":["#split epsiodes\n","#train = 120 episodes (~70%)\n","#valid = 35 epsiodes (20%)\n","#test = 18 episodes (10%)\n","\n","train_episodes = episodes_list[:120]\n","valid_episodes = episodes_list[120:155]\n","test_episodes = episodes_list[155:]"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"06Cdmjk9kjpG"},"source":["datasets ={}\n","\n","datasets['train'] = create_datasets(df, train_personas, train_episodes)\n","datasets['valid'] = create_datasets(df,test_personas, valid_episodes)\n","datasets['test'] = create_datasets(df, test_personas, test_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_fNIscls0Yr","executionInfo":{"status":"ok","timestamp":1606169049846,"user_tz":300,"elapsed":3311,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"87bcd00e-226a-40ee-847a-7e30e02337b4"},"source":["print('# examples in train: '+ str(len(datasets['train'])))\n","print('# examples in valid: '+ str(len(datasets['valid'])))\n","print('# examples in test: '+ str(len(datasets['test'])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# examples in train: 25997\n","# examples in valid: 7460\n","# examples in test: 4115\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFPEfvRhMQVS","executionInfo":{"status":"ok","timestamp":1606169049847,"user_tz":300,"elapsed":3305,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"152d0303-48af-46e0-d753-5bb4a705bc2a"},"source":["datasets['train'][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'char': 'GEORGE',\n"," 'labels': [\"well , why don ' t we just put a monitor in his skybox ?\"],\n"," 'text': 'your persona: i am very neurotic and always afraid that nobody likes me. \\nyour persona: i am selfish and greedy. \\nyour persona: i have low self-esteem. \\nyour persona: i am cheap.\\nno - one in the park is gonna be able to see it from there .'}"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"Sg2MR3WfGAZx"},"source":["# **Load PersonaChat**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INyKKk09GC7P","executionInfo":{"status":"ok","timestamp":1606169126643,"user_tz":300,"elapsed":3675,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"0883a8da-2ac6-4ba5-91de-c8270b41b5cb"},"source":["json_text = open('Seq2Seq_personas/train.jsonl', 'r').readlines()\n","train_chat = []\n","for chat in tqdm(json_text):\n","  chat = chat.rstrip()\n","  chat = json.loads(chat)\n","  train_chat.append(chat)\n","\n","json_text = open('Seq2Seq_personas/valid.jsonl', 'r').readlines()\n","valid_chat = []\n","for chat in tqdm(json_text):\n","  chat = chat.rstrip()\n","  chat = json.loads(chat)\n","  valid_chat.append(chat)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 131438/131438 [00:01<00:00, 99202.74it/s]\n","100%|██████████| 7801/7801 [00:00<00:00, 139887.84it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pEznu15LLYhY","executionInfo":{"status":"ok","timestamp":1606169126644,"user_tz":300,"elapsed":3661,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"08ae82a4-562e-4622-949e-581c4c76102d"},"source":["train_chat[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'episode_done': True,\n"," 'id': 'convai2:self:no_cands',\n"," 'labels': ['you must be very fast . hunting is one of my favorite hobbies .'],\n"," 'reward': 0,\n"," 'text': \"your persona: i like to remodel homes.\\nyour persona: i like to go hunting.\\nyour persona: i like to shoot a bow.\\nyour persona: my favorite holiday is halloween.\\nhi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape .\"}"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"qc-8xDpNHHDZ"},"source":["# **Combine Seinfeld + PersonaChat Datasets**"]},{"cell_type":"code","metadata":{"id":"yKkQI-_GGs3_"},"source":["for i in train_chat:\n","  datasets['train'].append(i)\n","\n","for i in valid_chat:\n","  datasets['valid'].append(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QSTGexYN-CRV"},"source":["# **Dictionary**"]},{"cell_type":"code","metadata":{"id":"vqOVG8RMM58E"},"source":["class Dictionary(object): #maps words to indices\n","    def __init__(self, datasets, include_valid=False):\n","        self.tokens = []\n","        self.ids = {}\n","        self.counts = {}\n","        \n","        # add special tokens\n","        self.add_token('__null__')\n","        self.add_token('__start__') #beginning of sentence\n","        self.add_token('__end__') #end of sentence\n","        self.add_token('__unk__') #unknown. Needed in case use with text with word that isn't in vocab\n","        \n","        RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)\n","\n","        for line in tqdm(datasets['train']):\n","            for w in RETOK.findall(line['labels'][0]):\n","                self.add_token(w)\n","            for w in RETOK.findall(line['text']):\n","                self.add_token(w)\n","                    \n","        if include_valid is True:\n","            for line in tqdm(datasets['valid']):\n","                if 'labels' in line:\n","                  for w in RETOK.findall(line['labels'][0]):\n","                      self.add_token(w)\n","                else:\n","                  for w in RETOK.findall(line['eval_labels'][0]):\n","                      self.add_token(w)\n","\n","                for w in RETOK.findall(line['text']):\n","                    self.add_token(w)\n","                            \n","    def add_token(self, w):\n","        if w not in self.tokens:\n","            self.tokens.append(w)\n","            _w_id = len(self.tokens) - 1\n","            self.ids[w] = _w_id\n","            self.counts[w] = 1\n","        else:\n","            self.counts[w] += 1\n","\n","    def get_id(self, w):\n","        return self.ids[w]\n","    \n","    def get_token(self, idx):\n","        return self.tokens[idx]\n","    \n","    def v2t(self, list_ids):\n","        #return [self.tokens[i] for i in l]\n","        return ' '.join([self.tokens[i] for i in list_ids])\n","    \n","    def t2v(self, tokenized_text):\n","        #return [self.ids[i] if i in self.ids else self.ids['__unk__'] for i in l]\n","        return [self.ids[w] if w in self.counts else self.ids['__unk__'] for w in tokenized_text]\n","    \n","    def pred2text(self, tensor):\n","      result = []\n","      for i in range(tensor.size(0)):\n","          if tensor[i].item() == '__end__'  or tensor[i].item() == '__null__':  # null is pad\n","              break\n","          else:\n","              result.append(self.tokens[tensor[i].item()])\n","      return ' '.join(result)\n","    \n","    def __len__(self):\n","        return len(self.tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMGdwKEmNEMh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606169514143,"user_tz":300,"elapsed":385727,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"3cabdff8-34cd-4924-db3b-61eae81ac628"},"source":["data_dict = Dictionary(datasets, include_valid=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 157435/157435 [05:50<00:00, 448.65it/s]\n","100%|██████████| 15261/15261 [00:34<00:00, 443.74it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"SwzcG7cj-G8i"},"source":["# **Dialogue Dataset**"]},{"cell_type":"code","metadata":{"id":"6h6q4EmG010v"},"source":["class DialogueDataset(Dataset):\n","    \"\"\"\n","    Json dataset wrapper\n","    \"\"\"\n","    \n","    def __init__(self, dataset, dictionary):\n","        super().__init__()\n","        \n","        self.samples = []\n","        \n","        for sample in tqdm(dataset):\n","            #sample = sample.rstrip()\n","            #sample = json.loads(sample)\n","            RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)\n","            _inp_toked = RETOK.findall(sample['text'])\n","            _inp_toked_id = dictionary.t2v(_inp_toked)\n","\n","            sample['text_vec'] = torch.tensor(_inp_toked_id, dtype=torch.long)\n","            \n","            #personachat valid labels are called 'eval_labels', Seinfeld valid labels are called 'labels'\n","            if 'labels' in sample:\n","              _tar_toked = RETOK.findall(sample['labels'][0]) + ['__end__']\n","            else:\n","              _tar_toked = RETOK.findall(sample['eval_labels'][0]) + ['__end__']\n","            _tar_toked_id = dictionary.t2v(_tar_toked)\n","            \n","            sample['target_vec'] = torch.tensor(_tar_toked_id, dtype=torch.long)\n","            \n","            self.samples.append(sample)\n","            \n","    def __getitem__(self, i):\n","        return self.samples[i]['text_vec'], self.samples[i]['target_vec']\n","    \n","    def __len__(self):\n","        return len(self.samples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8XeUM7oRpLl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606169569644,"user_tz":300,"elapsed":14266,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"87fc9a24-0150-484b-9a38-8bddbe32d046"},"source":["train_dataset = DialogueDataset(datasets['train'], data_dict)\n","valid_dataset = DialogueDataset(datasets['valid'], data_dict)\n","test_dataset = DialogueDataset(datasets['test'], data_dict)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 157435/157435 [00:12<00:00, 13005.82it/s]\n","100%|██████████| 15261/15261 [00:01<00:00, 14551.68it/s]\n","100%|██████████| 4115/4115 [00:00<00:00, 17808.95it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GwrtFKHkOYOn","executionInfo":{"status":"ok","timestamp":1606169569645,"user_tz":300,"elapsed":14260,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"46aa7e7d-40ac-4940-857b-53e799ddce09"},"source":["datasets['train'][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'char': 'GEORGE',\n"," 'labels': [\"well , why don ' t we just put a monitor in his skybox ?\"],\n"," 'target_vec': tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,  2]),\n"," 'text': 'your persona: i am very neurotic and always afraid that nobody likes me. \\nyour persona: i am selfish and greedy. \\nyour persona: i have low self-esteem. \\nyour persona: i am cheap.\\nno - one in the park is gonna be able to see it from there .',\n"," 'text_vec': tensor([19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 19, 20,\n","         21, 22, 23, 35, 26, 36, 33, 34, 19, 20, 21, 22, 37, 38, 39, 40, 41, 33,\n","         34, 19, 20, 21, 22, 23, 42, 33, 34, 43, 40, 44, 15, 45, 46, 47, 48, 49,\n","         50, 51, 52, 53, 54, 55, 33])}"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHaPJweUsqT3","executionInfo":{"status":"ok","timestamp":1606169569646,"user_tz":300,"elapsed":14255,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"2c819ae0-580b-4d1c-bc98-08c1243cbdc5"},"source":["train_dataset[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 19, 20,\n","         21, 22, 23, 35, 26, 36, 33, 34, 19, 20, 21, 22, 37, 38, 39, 40, 41, 33,\n","         34, 19, 20, 21, 22, 23, 42, 33, 34, 43, 40, 44, 15, 45, 46, 47, 48, 49,\n","         50, 51, 52, 53, 54, 55, 33]),\n"," tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,  2]))"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"gdH3mpBtUsVq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606169569946,"user_tz":300,"elapsed":14549,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"00029ca2-348a-4876-be51-05fee9056dc3"},"source":["print(data_dict.v2t(train_dataset[0][0].tolist()), \"\\n\\n\", data_dict.v2t(train_dataset[0][1].tolist()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["your persona : i am very neurotic and always afraid that nobody likes me . \n"," your persona : i am selfish and greedy . \n"," your persona : i have low self - esteem . \n"," your persona : i am cheap . \n"," no - one in the park is gonna be able to see it from there . \n","\n"," well , why don ' t we just put a monitor in his skybox ? __end__\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P0wN_CWvUMtu"},"source":["# **Padding, sorting, packing**"]},{"cell_type":"code","metadata":{"id":"hGLv5ktMT4me"},"source":["def pad_tensor(tensors, sort=True, pad_token=0):\n","    rows = len(tensors)\n","    lengths = [len(i) for i in tensors]\n","    max_t = max(lengths)\n","        \n","    output = tensors[0].new(rows, max_t)\n","    output.fill_(pad_token)  # 0 is a pad token here\n","    \n","    for i, (tensor, length) in enumerate(zip(tensors, lengths)):\n","        output[i,:length] = tensor\n","\n","    return output, lengths\n","\n","def argsort(keys, *lists, descending=False):\n","    \"\"\"Reorder each list in lists by the (descending) sorted order of keys.\n","    :param iter keys: Keys to order by.\n","    :param list[list] lists: Lists to reordered by keys's order.\n","                             Correctly handles lists and 1-D tensors.\n","    :param bool descending: Use descending order if true.\n","    :returns: The reordered items.\n","    \"\"\"\n","    ind_sorted = sorted(range(len(keys)), key=lambda k: keys[k])\n","    if descending:\n","        ind_sorted = list(reversed(ind_sorted))\n","    output = []\n","    for lst in lists:\n","        if isinstance(lst, torch.Tensor):\n","            output.append(lst[ind_sorted])\n","        else:\n","            output.append([lst[i] for i in ind_sorted])\n","    return output\n","\n","def batchify(batch):\n","    inputs = [i[0] for i in batch]\n","    labels = [i[1] for i in batch]\n","    \n","    input_vecs, input_lens = pad_tensor(inputs)\n","    label_vecs, label_lens = pad_tensor(labels)\n","    \n","    # sort only wrt inputs here for encoder packinng\n","    input_vecs, input_lens, label_vecs, label_lens = argsort(input_lens, input_vecs, input_lens, label_vecs, label_lens, descending=True)\n","\n","    return {\n","        \"text_vecs\": input_vecs,\n","        \"text_lens\": input_lens,\n","        \"target_vecs\": label_vecs,\n","        \"target_lens\": label_lens,\n","        'use_packed': True\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ex_xSQ1SlUBc"},"source":["train_loader = DataLoader(train_dataset, shuffle=True, collate_fn=batchify, batch_size=32)\n","valid_loader = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPmiacjoRw3i"},"source":["# **Model**"]},{"cell_type":"code","metadata":{"id":"mK-Mva3DFdDu"},"source":["class EncoderRNN(nn.Module):\n","    \"\"\"Encodes the input context.\"\"\"\n","\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx=0, dropout=0, shared_lt=None):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.pad_idx = pad_idx\n","        \n","        if shared_lt is None:\n","            self.embedding = nn.Embedding(self.vocab_size, self.embed_size, pad_idx)\n","        else:\n","            self.embedding = shared_lt\n","            \n","        self.gru = nn.GRU(\n","            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n","        )\n","        \n","        \n","    def forward(self, text_vec, text_lens, hidden=None, use_packed=True):\n","        embedded = self.embedding(text_vec)\n","        attention_mask = text_vec.ne(self.pad_idx)\n","\n","        embedded = self.dropout(embedded)\n","        if use_packed is True:\n","            embedded = pack_padded_sequence(embedded, text_lens, batch_first=True)\n","        output, hidden = self.gru(embedded, hidden)\n","        if use_packed is True:\n","            output, output_lens = pad_packed_sequence(output, batch_first=True)\n","        \n","        return output, hidden, attention_mask\n","\n","    \n","class DecoderRNN(nn.Module):\n","    \"\"\"Generates a sequence of tokens in response to context.\"\"\"\n","\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        self.embedding = nn.Embedding(self.vocab_size, self.embed_size, 0)\n","        \n","        self.gru = nn.GRU(\n","            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n","        )\n","        \n","        self.attention = AttentionLayer(self.hidden_size, self.embed_size)\n","\n","        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n","        self.longest_label = 100\n","\n","    def forward(self, text_vec, decoder_hidden, encoder_states):\n","        emb = self.embedding(text_vec)\n","        emb = self.dropout(emb)\n","        seqlen = text_vec.size(1)\n","        encoder_output, encoder_hidden, attention_mask = encoder_states\n","        \n","        decoder_hidden = decoder_hidden\n","        output = []\n","        attn_w_log = []\n","\n","        for i in range(seqlen):\n","            decoder_output, decoder_hidden = self.gru(emb[:,i,:].unsqueeze(1), decoder_hidden)\n","            \n","            # compute attention at each time step\n","            decoder_output_attended, attn_weights = self.attention(decoder_output, decoder_hidden, encoder_output, attention_mask)\n","            output.append(decoder_output_attended)\n","            attn_w_log.append(attn_weights)\n","            \n","        output = torch.cat(output, dim=1).to(text_vec.device)\n","        scores = self.out(output)\n","        \n","        return scores, decoder_hidden, attn_w_log\n","    \n","    def decode_forced(self, ys, encoder_states, xs_lens):\n","        encoder_output, encoder_hidden, attention_mask = encoder_states\n","        \n","        batch_size = ys.size(0)\n","        target_length = ys.size(1)\n","        longest_label = max(target_length, self.longest_label)\n","        \n","        starts = torch.Tensor([1]).long().to(self.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n","        \n","        # Teacher forcing: Feed the target as the next input\n","        y_in = ys.narrow(1, 0, ys.size(1) - 1)\n","        decoder_input = torch.cat([starts, y_in], 1)\n","        decoder_output, decoder_hidden, attn_w_log = self.forward(decoder_input, encoder_hidden, encoder_states)\n","        _, preds = decoder_output.max(dim=2)\n","        \n","        return decoder_output, preds, attn_w_log\n","    \n","    \n","class AttentionLayer(nn.Module):\n","\n","    def __init__(self, hidden_size, embedding_size):\n","        super().__init__()\n","        input_dim = hidden_size\n","\n","        self.linear_out = nn.Linear(hidden_size+input_dim, input_dim, bias=False)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, decoder_output, decoder_hidden, encoder_output, attention_mask):\n","\n","        batch_size, seq_length, hidden_size = encoder_output.size()\n","\n","        encoder_output_t = encoder_output.transpose(1,2)\n","        \n","        attention_scores = torch.bmm(decoder_output, encoder_output_t).squeeze(1)\n","\n","        attention_scores.masked_fill_((~attention_mask), -10e5)\n","        attention_weights = self.softmax(attention_scores)\n","\n","        mix = torch.bmm(attention_weights.unsqueeze(1), encoder_output)\n","\n","        combined = torch.cat((decoder_output.squeeze(1), mix.squeeze(1)), dim=1)\n","\n","        output = self.linear_out(combined).unsqueeze(1)\n","        output = self.tanh(output)\n","\n","        return output, attention_weights\n","    \n","    \n","class seq2seq(nn.Module):\n","    \"\"\"\n","    Generic seq2seq model with attention mechanism.\n","    \"\"\"\n","    def __init__(self, opts):\n","\n","        super().__init__()\n","        self.opts = opts\n","        \n","        self.decoder = DecoderRNN(\n","                                    vocab_size=self.opts['vocab_size'],\n","                                    embed_size=self.opts['embedding_size'],\n","                                    hidden_size=self.opts['hidden_size'],\n","                                    num_layers=self.opts['num_layers_dec'],\n","                                    dropout=self.opts['dropout'],\n","                                )\n","        \n","        self.encoder = EncoderRNN(\n","                                    vocab_size=self.opts['vocab_size'],\n","                                    embed_size=self.opts['embedding_size'],\n","                                    hidden_size=self.opts['hidden_size'],\n","                                    num_layers=self.opts['num_layers_enc'],\n","                                    dropout=self.opts['dropout'],\n","                                    shared_lt=self.decoder.embedding\n","        )\n","        \n","    def train(self):\n","        self.encoder.train()\n","        self.decoder.train()\n","        \n","    def eval(self):\n","        self.encoder.eval()\n","        self.decoder.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9x2MZ2xFhan"},"source":["num_gpus = torch.cuda.device_count()\n","\n","if num_gpus > 0:\n","    current_device = 'cuda'\n","else:\n","    current_device = 'cpu'\n","\n","load_pretrained = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0zO3RIWFnV-"},"source":["if load_pretrained is True:\n","    model_pt = torch.load('Seq2Seq_personas/seq2seq_checkpts/PHASE_1/model_best_10.pt', map_location=torch.device(current_device))\n","    \n","    opts = model_pt['opts']\n","    \n","    model = seq2seq(opts)\n","    model.load_state_dict(model_pt['state_dict'])\n","    model.to(current_device)\n","    plot_cache = model_pt['plot_cache']\n","    \n","else:\n","    \n","    opts = {}\n","\n","    opts['vocab_size'] = len(data_dict)\n","    opts['hidden_size'] = 512\n","    opts['embedding_size'] = 256\n","    opts['num_layers_enc'] = 2\n","    opts['num_layers_dec'] = 2\n","    opts['dropout'] = 0.3\n","    opts['encoder_shared_lt'] = True\n","\n","    model = seq2seq(opts)\n","    model.to(current_device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uxYgGy9GrEC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606177540089,"user_tz":300,"elapsed":377,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"5f2556a9-3df4-4d83-a03a-11a4d36dc602"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["seq2seq(\n","  (decoder): DecoderRNN(\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (embedding): Embedding(26307, 256, padding_idx=0)\n","    (gru): GRU(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n","    (attention): AttentionLayer(\n","      (linear_out): Linear(in_features=1024, out_features=512, bias=False)\n","      (softmax): Softmax(dim=-1)\n","      (tanh): Tanh()\n","    )\n","    (out): Linear(in_features=512, out_features=26307, bias=True)\n","  )\n","  (encoder): EncoderRNN(\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (embedding): Embedding(26307, 256, padding_idx=0)\n","    (gru): GRU(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"81hzV1kFWzTy"},"source":["criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n","optimizer = torch.optim.Adam(model.parameters(), 0.001, amsgrad=True)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f0OqI0OzSMD9"},"source":["# **Training Setup**"]},{"cell_type":"code","metadata":{"id":"pTk-vKVN1cX8"},"source":["def restore_model(filename, model, optimizer, scheduler):\n","\n","  #restore model from previous saved state for fine-tuning or continuing to train\n","\n","    if os.path.isfile(filename):\n","        print(\"=> loading checkpoint '{}'\".format(filename))\n","\n","        model_pt = torch.load(filename, map_location=torch.device(current_device))\n","        opts = model_pt['opts']\n","    \n","        model.load_state_dict(model_pt['state_dict'])\n","        model.to(current_device)\n","\n","        optimizer.load_state_dict(model_pt['optimizer'])\n","        scheduler.load_state_dict(model_pt['scheduler'])\n","\n","        plot_cache = model_pt['plot_cache']\n","        best_val_loss = 100\n","\n","        #get best val loss from where model left off\n","        for i in plot_cache:\n","          if i[1] < best_val_loss:\n","            best_val_loss = i[1]\n","\n","        print(\"=> loaded model\")\n","    else:\n","        print(\"=> no checkpoint found\")\n","    \n","    return model, plot_cache, optimizer, scheduler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hU3youIMW4Hy"},"source":["def train(model, optimizer, scheduler, start_epoch, train_loader, valid_loader, plot_cache =[], best_val_loss = 100):\n","\n","    t = range(start_epoch,100)\n","\n","    for epoch in t:\n","        \n","        model.train()\n","        sum_loss = 0\n","        sum_tokens = 0\n","        \n","        for i, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            \n","            text_vecs = batch['text_vecs'].to('cuda')\n","            target_vecs = batch['target_vecs'].to('cuda')\n","            \n","            encoded = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n","            \n","            decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n","            \n","            scores = decoder_output.view(-1, decoder_output.size(-1))\n","            \n","            loss = criterion(scores, target_vecs.view(-1))\n","            \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            sum_loss += loss.item()\n","            \n","            num_tokens = target_vecs.ne(0).long().sum().item()\n","            loss /= num_tokens\n","            \n","            sum_tokens += num_tokens\n","\n","            optimizer.step()\n","            \n","            if i % 100 == 0:\n","                avg_train_loss = sum_loss/sum_tokens\n","                print(\"iter {} train loss = {}\".format(i, avg_train_loss))\n","                \n","        val_loss = 0\n","        val_tokens = 0\n","        for i, batch in enumerate(valid_loader):\n","            model.eval()\n","            \n","            text_vecs = batch['text_vecs'].to('cuda')\n","            target_vecs = batch['target_vecs'].to('cuda')\n","            \n","            encoded = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n","            \n","            decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n","            \n","            scores = decoder_output.view(-1, decoder_output.size(-1))\n","            \n","            loss = criterion(scores, target_vecs.view(-1))\n","            \n","            num_tokens = target_vecs.ne(0).long().sum().item()\n","            val_loss += loss.item()\n","            \n","            val_tokens += num_tokens\n","            \n","        avg_val_loss = val_loss/val_tokens\n","        scheduler.step(avg_val_loss)\n","        \n","        print(\"Epoch {} valid loss = {}\".format(epoch, avg_val_loss))\n","        \n","        plot_cache.append( (avg_train_loss, avg_val_loss) )\n","        \n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            \n","            torch.save({\n","            'state_dict': model.state_dict(),\n","            'opts': opts,\n","            'plot_cache': plot_cache,\n","            'optimizer': optimizer.state_dict(),\n","            'scheduler': scheduler.state_dict(),\n","                }, f'Seq2Seq_personas/seq2seq_checkpts/PHASE_1/model_best_{epoch}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"W5Hr-A9r4GFo","executionInfo":{"status":"error","timestamp":1606197709458,"user_tz":300,"elapsed":20157092,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"1dee99ef-ccca-4ed1-f1ee-88c047768f41"},"source":["#to continue or start training\n","restore = False\n","\n","if restore is True:\n","  model, plot_cache, optimizer, scheduler = restore_model('Seq2Seq_personas/seq2seq_checkpts/PHASE_1/model_best_3.pt', seq2seq(opts), optimizer, scheduler)\n","  train(model, optimizer, scheduler, 4, train_loader, valid_loader, plot_cache)\n","\n","else:\n","  train(model, optimizer, scheduler, 0, train_loader, valid_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["iter 0 train loss = 10.180068490751122\n","iter 100 train loss = 6.157275787474823\n","iter 200 train loss = 5.765740143893466\n","iter 300 train loss = 5.478438432736212\n","iter 400 train loss = 5.277362155641719\n","iter 500 train loss = 5.136364660679579\n","iter 600 train loss = 5.0302153212837375\n","iter 700 train loss = 4.941906409186733\n","iter 800 train loss = 4.864209222802406\n","iter 900 train loss = 4.799952110912306\n","iter 1000 train loss = 4.7441818752911455\n","iter 1100 train loss = 4.6992602189955575\n","iter 1200 train loss = 4.659762639574291\n","iter 1300 train loss = 4.6230628102738\n","iter 1400 train loss = 4.591146053574645\n","iter 1500 train loss = 4.559300847734724\n","iter 1600 train loss = 4.533220674257773\n","iter 1700 train loss = 4.508323410927093\n","iter 1800 train loss = 4.482174910656818\n","iter 1900 train loss = 4.461287863201406\n","iter 2000 train loss = 4.440152124184329\n","iter 2100 train loss = 4.421023091901925\n","iter 2200 train loss = 4.4026356714411055\n","iter 2300 train loss = 4.3854898730420935\n","iter 2400 train loss = 4.370597221875269\n","iter 2500 train loss = 4.35579214483909\n","iter 2600 train loss = 4.341691401274984\n","iter 2700 train loss = 4.3300173245914415\n","iter 2800 train loss = 4.317927025202756\n","iter 2900 train loss = 4.304287347585033\n","iter 3000 train loss = 4.294797647067808\n","iter 3100 train loss = 4.283040081436457\n","iter 3200 train loss = 4.272426067226779\n","iter 3300 train loss = 4.262504041039887\n","iter 3400 train loss = 4.253521935970489\n","iter 3500 train loss = 4.245010518825084\n","iter 3600 train loss = 4.236884885478884\n","iter 3700 train loss = 4.228936607133037\n","iter 3800 train loss = 4.221301936555376\n","iter 3900 train loss = 4.212992375290355\n","iter 4000 train loss = 4.205606020802301\n","iter 4100 train loss = 4.197883006004003\n","iter 4200 train loss = 4.19063509642392\n","iter 4300 train loss = 4.184135518121718\n","iter 4400 train loss = 4.177994173195845\n","iter 4500 train loss = 4.17128520796193\n","iter 4600 train loss = 4.165661816650999\n","iter 4700 train loss = 4.159192523031846\n","iter 4800 train loss = 4.153405957935318\n","iter 4900 train loss = 4.147873945746549\n","Epoch 0 valid loss = 4.138894481098549\n","iter 0 train loss = 3.9156637752757355\n","iter 100 train loss = 3.710197107661781\n","iter 200 train loss = 3.711541182141293\n","iter 300 train loss = 3.70947744489771\n","iter 400 train loss = 3.7059297024047573\n","iter 500 train loss = 3.7053677299921355\n","iter 600 train loss = 3.711831206240241\n","iter 700 train loss = 3.712636140776133\n","iter 800 train loss = 3.7147833971677207\n","iter 900 train loss = 3.7158693197906723\n","iter 1000 train loss = 3.7182618961909513\n","iter 1100 train loss = 3.720266730630687\n","iter 1200 train loss = 3.7213895635834877\n","iter 1300 train loss = 3.722656812960956\n","iter 1400 train loss = 3.721863600225956\n","iter 1500 train loss = 3.72129571864661\n","iter 1600 train loss = 3.723276053176673\n","iter 1700 train loss = 3.724017596958301\n","iter 1800 train loss = 3.7250325912258955\n","iter 1900 train loss = 3.7244213966232906\n","iter 2000 train loss = 3.7248395117681015\n","iter 2100 train loss = 3.726079313459942\n","iter 2200 train loss = 3.7262428708316784\n","iter 2300 train loss = 3.7267126336512035\n","iter 2400 train loss = 3.7278565604840606\n","iter 2500 train loss = 3.7282207502482043\n","iter 2600 train loss = 3.7285629735045664\n","iter 2700 train loss = 3.7302213085070552\n","iter 2800 train loss = 3.7298234440505817\n","iter 2900 train loss = 3.7314401388975114\n","iter 3000 train loss = 3.7316848792568975\n","iter 3100 train loss = 3.730648320386879\n","iter 3200 train loss = 3.7300014804474886\n","iter 3300 train loss = 3.730095263443865\n","iter 3400 train loss = 3.7297995401680093\n","iter 3500 train loss = 3.730011414193757\n","iter 3600 train loss = 3.729721280259235\n","iter 3700 train loss = 3.7297417898502854\n","iter 3800 train loss = 3.7293084961524876\n","iter 3900 train loss = 3.7294015493954227\n","iter 4000 train loss = 3.729343171460037\n","iter 4100 train loss = 3.730908200082453\n","iter 4200 train loss = 3.729803838591252\n","iter 4300 train loss = 3.7294426502929787\n","iter 4400 train loss = 3.729505722088306\n","iter 4500 train loss = 3.729047012934548\n","iter 4600 train loss = 3.7285553511092044\n","iter 4700 train loss = 3.7277959996120935\n","iter 4800 train loss = 3.728161520506392\n","iter 4900 train loss = 3.7279247906188755\n","Epoch 1 valid loss = 4.11233548027785\n","iter 0 train loss = 3.4261045384585413\n","iter 100 train loss = 3.5426424587711938\n","iter 200 train loss = 3.5498786989292617\n","iter 300 train loss = 3.5397020259595746\n","iter 400 train loss = 3.550149570588846\n","iter 500 train loss = 3.5552390720672666\n","iter 600 train loss = 3.551397747690697\n","iter 700 train loss = 3.5530969955727962\n","iter 800 train loss = 3.5506821692712394\n","iter 900 train loss = 3.553879353156992\n","iter 1000 train loss = 3.5566175105558546\n","iter 1100 train loss = 3.5561061327785195\n","iter 1200 train loss = 3.557419663338753\n","iter 1300 train loss = 3.5592359932220416\n","iter 1400 train loss = 3.561919647214549\n","iter 1500 train loss = 3.564513626574558\n","iter 1600 train loss = 3.5640992397640328\n","iter 1700 train loss = 3.5665450978212965\n","iter 1800 train loss = 3.567135282976679\n","iter 1900 train loss = 3.567832644483211\n","iter 2000 train loss = 3.569747438763343\n","iter 2100 train loss = 3.5714335215040633\n","iter 2200 train loss = 3.570575360587103\n","iter 2300 train loss = 3.5723837309195967\n","iter 2400 train loss = 3.5749692036160607\n","iter 2500 train loss = 3.576566631732193\n","iter 2600 train loss = 3.5774407712067475\n","iter 2700 train loss = 3.579411935327296\n","iter 2800 train loss = 3.5807031780953347\n","iter 2900 train loss = 3.580809341450793\n","iter 3000 train loss = 3.5833861433341467\n","iter 3100 train loss = 3.5837339784221327\n","iter 3200 train loss = 3.584993441061438\n","iter 3300 train loss = 3.585580212569595\n","iter 3400 train loss = 3.587442560156424\n","iter 3500 train loss = 3.588705861440287\n","iter 3600 train loss = 3.589801531896731\n","iter 3700 train loss = 3.5884807129726393\n","iter 3800 train loss = 3.589484268701475\n","iter 3900 train loss = 3.589558366268284\n","iter 4000 train loss = 3.59007381475657\n","iter 4100 train loss = 3.590116892361006\n","iter 4200 train loss = 3.5906574178749584\n","iter 4300 train loss = 3.590510423594857\n","iter 4400 train loss = 3.5910659871184443\n","iter 4500 train loss = 3.5910482145389797\n","iter 4600 train loss = 3.592437154505509\n","iter 4700 train loss = 3.592927129762745\n","iter 4800 train loss = 3.5938814105002166\n","iter 4900 train loss = 3.594978711054061\n","Epoch 2 valid loss = 4.138435699187691\n","iter 0 train loss = 3.4952725227151764\n","iter 100 train loss = 3.453057950372792\n","iter 200 train loss = 3.4471993951455895\n","iter 300 train loss = 3.446195444651741\n","iter 400 train loss = 3.4571018135213363\n","iter 500 train loss = 3.4539871109295897\n","iter 600 train loss = 3.455648139328277\n","iter 700 train loss = 3.461017879384234\n","iter 800 train loss = 3.4600739474103883\n","iter 900 train loss = 3.4595960431645985\n","iter 1000 train loss = 3.4659827314455263\n","iter 1100 train loss = 3.4700264848880513\n","iter 1200 train loss = 3.4698506547558647\n","iter 1300 train loss = 3.4725516922906605\n","iter 1400 train loss = 3.4724753265255175\n","iter 1500 train loss = 3.4751529291974066\n","iter 1600 train loss = 3.4768653242543697\n","iter 1700 train loss = 3.4782790001294055\n","iter 1800 train loss = 3.4790695196782777\n","iter 1900 train loss = 3.480722227081027\n","iter 2000 train loss = 3.481770458865136\n","iter 2100 train loss = 3.483816938964098\n","iter 2200 train loss = 3.485205078442958\n","iter 2300 train loss = 3.486677306072511\n","iter 2400 train loss = 3.488264477270533\n","iter 2500 train loss = 3.489773944322614\n","iter 2600 train loss = 3.489851371536073\n","iter 2700 train loss = 3.4905070709741857\n","iter 2800 train loss = 3.4926136208814724\n","iter 2900 train loss = 3.49446026205391\n","iter 3000 train loss = 3.4955605845263293\n","iter 3100 train loss = 3.4966791088323266\n","iter 3200 train loss = 3.4978765526195583\n","iter 3300 train loss = 3.4996321924482747\n","iter 3400 train loss = 3.5008884516509085\n","iter 3500 train loss = 3.5013961650943446\n","iter 3600 train loss = 3.50249711002724\n","iter 3700 train loss = 3.5033480016235075\n","iter 3800 train loss = 3.5047432333151383\n","iter 3900 train loss = 3.504776288944542\n","iter 4000 train loss = 3.505332943518845\n","iter 4100 train loss = 3.506444985272785\n","iter 4200 train loss = 3.507679012060832\n","iter 4300 train loss = 3.5083755642442664\n","iter 4400 train loss = 3.509189121906737\n","iter 4500 train loss = 3.5102404005643626\n","iter 4600 train loss = 3.511840382733063\n","iter 4700 train loss = 3.51262472943004\n","iter 4800 train loss = 3.5125265771662892\n","iter 4900 train loss = 3.513105748523706\n","Epoch 3 valid loss = 4.160975129863496\n","iter 0 train loss = 3.192933881874184\n","iter 100 train loss = 3.370927024610115\n","iter 200 train loss = 3.364183862254845\n","iter 300 train loss = 3.3733184700805854\n","iter 400 train loss = 3.381553079301984\n","iter 500 train loss = 3.3849001750320333\n","iter 600 train loss = 3.397122129813865\n","iter 700 train loss = 3.401000703543874\n","iter 800 train loss = 3.4035796697046634\n","iter 900 train loss = 3.405953083076088\n","iter 1000 train loss = 3.4094865874984244\n","iter 1100 train loss = 3.4093545785613757\n","iter 1200 train loss = 3.411038060423964\n","iter 1300 train loss = 3.410166773600808\n","iter 1400 train loss = 3.412548323863706\n","iter 1500 train loss = 3.415113964036294\n","iter 1600 train loss = 3.417913849336221\n","iter 1700 train loss = 3.4183837234675774\n","iter 1800 train loss = 3.42089370463444\n","iter 1900 train loss = 3.422661256589676\n","iter 2000 train loss = 3.4234930221734885\n","iter 2100 train loss = 3.424445256339144\n","iter 2200 train loss = 3.4257903970799126\n","iter 2300 train loss = 3.4269229560623673\n","iter 2400 train loss = 3.427306126384389\n","iter 2500 train loss = 3.4293054431739973\n","iter 2600 train loss = 3.4310125240935614\n","iter 2700 train loss = 3.431699674971965\n","iter 2800 train loss = 3.4344770035169607\n","iter 2900 train loss = 3.435591747809146\n","iter 3000 train loss = 3.4364173585542703\n","iter 3100 train loss = 3.438511200207788\n","iter 3200 train loss = 3.439568494711644\n","iter 3300 train loss = 3.4402653958914637\n","iter 3400 train loss = 3.4410900815292598\n","iter 3500 train loss = 3.442178419176249\n","iter 3600 train loss = 3.443522639011411\n","iter 3700 train loss = 3.4439590043193475\n","iter 3800 train loss = 3.444536049982429\n","iter 3900 train loss = 3.4455172584387586\n","iter 4000 train loss = 3.4469944365267606\n","iter 4100 train loss = 3.4481680182479604\n","iter 4200 train loss = 3.448563716347891\n","iter 4300 train loss = 3.448766951456382\n","iter 4400 train loss = 3.449944710458516\n","iter 4500 train loss = 3.4513945435928486\n","iter 4600 train loss = 3.4525415749456245\n","iter 4700 train loss = 3.4541028647826773\n","iter 4800 train loss = 3.455041549187432\n","iter 4900 train loss = 3.455485971175503\n","Epoch 4 valid loss = 4.207893403096839\n","iter 0 train loss = 3.262683618848569\n","iter 100 train loss = 3.293112997828859\n","iter 200 train loss = 3.3275280706649437\n","iter 300 train loss = 3.335144514072243\n","iter 400 train loss = 3.338142748892294\n","iter 500 train loss = 3.345457916403475\n","iter 600 train loss = 3.3487972520697964\n","iter 700 train loss = 3.3466323953839625\n","iter 800 train loss = 3.3493972736682704\n","iter 900 train loss = 3.350501096691803\n","iter 1000 train loss = 3.355017312635606\n","iter 1100 train loss = 3.356569307570085\n","iter 1200 train loss = 3.358855815930244\n","iter 1300 train loss = 3.3599541302399856\n","iter 1400 train loss = 3.3602061489150246\n","iter 1500 train loss = 3.361986518094558\n","iter 1600 train loss = 3.364465683464147\n","iter 1700 train loss = 3.3659929549583354\n","iter 1800 train loss = 3.368364868788875\n","iter 1900 train loss = 3.3689561300787463\n","iter 2000 train loss = 3.3703268851630632\n","iter 2100 train loss = 3.3729826777560996\n","iter 2200 train loss = 3.375076411142044\n","iter 2300 train loss = 3.3775441369291554\n","iter 2400 train loss = 3.3803895248847313\n","iter 2500 train loss = 3.381803592094139\n","iter 2600 train loss = 3.382825940262272\n","iter 2700 train loss = 3.3841414252228996\n","iter 2800 train loss = 3.3856812190170484\n","iter 2900 train loss = 3.3866725623670715\n","iter 3000 train loss = 3.3885299842044554\n","iter 3100 train loss = 3.3895766813885704\n","iter 3200 train loss = 3.390780049849674\n","iter 3300 train loss = 3.3922116377076836\n","iter 3400 train loss = 3.393843593913131\n","iter 3500 train loss = 3.394571142855653\n","iter 3600 train loss = 3.3954590955903683\n","iter 3700 train loss = 3.397205026298606\n","iter 3800 train loss = 3.3977994091127544\n","iter 3900 train loss = 3.3990610478143855\n","iter 4000 train loss = 3.39990631515097\n","iter 4100 train loss = 3.400679127529542\n","iter 4200 train loss = 3.401781382357316\n","iter 4300 train loss = 3.403038042676441\n","iter 4400 train loss = 3.404129859481461\n","iter 4500 train loss = 3.4052686846885933\n","iter 4600 train loss = 3.405690988449845\n","iter 4700 train loss = 3.4063567498553593\n","iter 4800 train loss = 3.407608045869037\n","iter 4900 train loss = 3.4084729730761416\n","Epoch 5 valid loss = 4.223762529813896\n","iter 0 train loss = 3.3399781780478395\n","iter 100 train loss = 3.296448905458721\n","iter 200 train loss = 3.2925273873353653\n","iter 300 train loss = 3.288454294282501\n","iter 400 train loss = 3.2928455585623877\n","iter 500 train loss = 3.2970484758366463\n","iter 600 train loss = 3.300510966600678\n","iter 700 train loss = 3.3046782008934805\n","iter 800 train loss = 3.3091546953301774\n","iter 900 train loss = 3.3112058359084413\n","iter 1000 train loss = 3.3150556663304185\n","iter 1100 train loss = 3.3176177099770303\n","iter 1200 train loss = 3.319656296428971\n","iter 1300 train loss = 3.31920264326699\n","iter 1400 train loss = 3.319550236004658\n","iter 1500 train loss = 3.3213132347852543\n","iter 1600 train loss = 3.3230616041255723\n","iter 1700 train loss = 3.3261478213428965\n","iter 1800 train loss = 3.3312511589217855\n","iter 1900 train loss = 3.3336338107919135\n","iter 2000 train loss = 3.3344259420925577\n","iter 2100 train loss = 3.336303027757321\n","iter 2200 train loss = 3.3371682373432185\n","iter 2300 train loss = 3.3387406564455127\n","iter 2400 train loss = 3.3404411652379538\n","iter 2500 train loss = 3.342693737022129\n","iter 2600 train loss = 3.344707325317832\n","iter 2700 train loss = 3.3461294133356994\n","iter 2800 train loss = 3.3476202534289157\n","iter 2900 train loss = 3.349721006110554\n","iter 3000 train loss = 3.350756408631757\n","iter 3100 train loss = 3.3518496775648847\n","iter 3200 train loss = 3.35284327645726\n","iter 3300 train loss = 3.354273535534551\n","iter 3400 train loss = 3.3559996781930974\n","iter 3500 train loss = 3.3571802254671987\n","iter 3600 train loss = 3.3583397696156987\n","iter 3700 train loss = 3.3596135103349556\n","iter 3800 train loss = 3.3600008092504865\n","iter 3900 train loss = 3.361585907236177\n","iter 4000 train loss = 3.361758657451579\n","iter 4100 train loss = 3.3625023933588176\n","iter 4200 train loss = 3.3628406953166885\n","iter 4300 train loss = 3.3644682521737854\n","iter 4400 train loss = 3.3654764349013764\n","iter 4500 train loss = 3.366557133628677\n","iter 4600 train loss = 3.3674412911306386\n","iter 4700 train loss = 3.3683114506762153\n","iter 4800 train loss = 3.3690624507308344\n","iter 4900 train loss = 3.370204119893992\n","Epoch 6 valid loss = 4.24098765878965\n","iter 0 train loss = 3.221810921267594\n","iter 100 train loss = 3.2531671639719897\n","iter 200 train loss = 3.2746468928113\n","iter 300 train loss = 3.277514399931938\n","iter 400 train loss = 3.2838627434835117\n","iter 500 train loss = 3.284122859822547\n","iter 600 train loss = 3.2769459609740053\n","iter 700 train loss = 3.278884378970791\n","iter 800 train loss = 3.2844444482641943\n","iter 900 train loss = 3.2849480747466817\n","iter 1000 train loss = 3.2857548666622365\n","iter 1100 train loss = 3.287941865198774\n","iter 1200 train loss = 3.2906896146511193\n","iter 1300 train loss = 3.294431225300822\n","iter 1400 train loss = 3.297020894652723\n","iter 1500 train loss = 3.2993638755972468\n","iter 1600 train loss = 3.300505352519561\n","iter 1700 train loss = 3.302214182634965\n","iter 1800 train loss = 3.3025822344183497\n","iter 1900 train loss = 3.3037282758411677\n","iter 2000 train loss = 3.3060591874008933\n","iter 2100 train loss = 3.30848224178855\n","iter 2200 train loss = 3.3086510703677616\n","iter 2300 train loss = 3.3097274618811467\n","iter 2400 train loss = 3.3118047698044073\n","iter 2500 train loss = 3.310850062561319\n","iter 2600 train loss = 3.3125421180431154\n","iter 2700 train loss = 3.3141780527852966\n","iter 2800 train loss = 3.314992603423244\n","iter 2900 train loss = 3.3168242088524478\n","iter 3000 train loss = 3.3180599545848763\n","iter 3100 train loss = 3.3185204038688867\n","iter 3200 train loss = 3.3195736495118444\n","iter 3300 train loss = 3.320636228192689\n","iter 3400 train loss = 3.322255295210461\n","iter 3500 train loss = 3.3240943267523764\n","iter 3600 train loss = 3.3246221054344574\n","iter 3700 train loss = 3.3264501197433436\n","iter 3800 train loss = 3.327941742226964\n","iter 3900 train loss = 3.3291558267590777\n","iter 4000 train loss = 3.3306628496306527\n","iter 4100 train loss = 3.33128304154265\n","iter 4200 train loss = 3.332294095996869\n","iter 4300 train loss = 3.3329801079958545\n","iter 4400 train loss = 3.3342862214192817\n","iter 4500 train loss = 3.3352882728695947\n","iter 4600 train loss = 3.3357175998819297\n","iter 4700 train loss = 3.336966662884739\n","iter 4800 train loss = 3.337407535466939\n","iter 4900 train loss = 3.3389140612979817\n","Epoch 7 valid loss = 4.229513590344862\n","iter 0 train loss = 3.034807134904791\n","iter 100 train loss = 3.2058114356317815\n","iter 200 train loss = 3.189561413970606\n","iter 300 train loss = 3.1852383713891794\n","iter 400 train loss = 3.177255719346501\n","iter 500 train loss = 3.1780742510631574\n","iter 600 train loss = 3.174554390301381\n","iter 700 train loss = 3.1732143613233292\n","iter 800 train loss = 3.1697513927100496\n","iter 900 train loss = 3.168641566362882\n","iter 1000 train loss = 3.1681761148634244\n","iter 1100 train loss = 3.1666035459893185\n","iter 1200 train loss = 3.165646625822357\n","iter 1300 train loss = 3.162332536155627\n","iter 1400 train loss = 3.160914931405418\n","iter 1500 train loss = 3.1576094870700038\n","iter 1600 train loss = 3.1576798533301034\n","iter 1700 train loss = 3.1567757453612937\n","iter 1800 train loss = 3.155574079273941\n","iter 1900 train loss = 3.1547953784606775\n","iter 2000 train loss = 3.154541619130179\n","iter 2100 train loss = 3.1541397756414082\n","iter 2200 train loss = 3.153135179745277\n","iter 2300 train loss = 3.1514933216633967\n","iter 2400 train loss = 3.1507090811218035\n","iter 2500 train loss = 3.1509569169371554\n","iter 2600 train loss = 3.1512371457733352\n","iter 2700 train loss = 3.149732837103008\n","iter 2800 train loss = 3.1494968875820626\n","iter 2900 train loss = 3.1487451495836325\n","iter 3000 train loss = 3.1477242182809415\n","iter 3100 train loss = 3.1467943172678665\n","iter 3200 train loss = 3.1457004953116114\n","iter 3300 train loss = 3.145200150462244\n","iter 3400 train loss = 3.143983470201105\n","iter 3500 train loss = 3.1438961952187485\n","iter 3600 train loss = 3.143861587204075\n","iter 3700 train loss = 3.142612560456963\n","iter 3800 train loss = 3.1424000641859022\n","iter 3900 train loss = 3.142199912197287\n","iter 4000 train loss = 3.142761250608284\n","iter 4100 train loss = 3.142128635698478\n","iter 4200 train loss = 3.142271899065682\n","iter 4300 train loss = 3.141780160726311\n","iter 4400 train loss = 3.1418475980658576\n","iter 4500 train loss = 3.1415966088500364\n","iter 4600 train loss = 3.141710345324044\n","iter 4700 train loss = 3.141466732405909\n","iter 4800 train loss = 3.1414876181181723\n","iter 4900 train loss = 3.141377050654717\n","Epoch 8 valid loss = 4.151156812697623\n","iter 0 train loss = 2.8605565121299343\n","iter 100 train loss = 3.0892643770636243\n","iter 200 train loss = 3.0867087927048464\n","iter 300 train loss = 3.0862297942215084\n","iter 400 train loss = 3.0943915803186766\n","iter 500 train loss = 3.0959756323455423\n","iter 600 train loss = 3.0945284133083844\n","iter 700 train loss = 3.094021137551544\n","iter 800 train loss = 3.091465673952399\n","iter 900 train loss = 3.0922561437489358\n","iter 1000 train loss = 3.0902593771793136\n","iter 1100 train loss = 3.090849891135761\n","iter 1200 train loss = 3.090618527532234\n","iter 1300 train loss = 3.089746890907584\n","iter 1400 train loss = 3.0882417794992163\n","iter 1500 train loss = 3.0890660383048623\n","iter 1600 train loss = 3.089668837468045\n","iter 1700 train loss = 3.0909627082726345\n","iter 1800 train loss = 3.0895758436905827\n","iter 1900 train loss = 3.0904099918909576\n","iter 2000 train loss = 3.090581030754574\n","iter 2100 train loss = 3.091144429068544\n","iter 2200 train loss = 3.091490124079915\n","iter 2300 train loss = 3.0919247413596405\n","iter 2400 train loss = 3.0917854954674358\n","iter 2500 train loss = 3.091714486202258\n","iter 2600 train loss = 3.0915027401187527\n","iter 2700 train loss = 3.0921480751280472\n","iter 2800 train loss = 3.09210400362344\n","iter 2900 train loss = 3.0918063512385143\n","iter 3000 train loss = 3.091469749191514\n","iter 3100 train loss = 3.0905594226471496\n","iter 3200 train loss = 3.091274234398039\n","iter 3300 train loss = 3.090424846965064\n","iter 3400 train loss = 3.0909310477554732\n","iter 3500 train loss = 3.090516229662961\n","iter 3600 train loss = 3.0911651963799183\n","iter 3700 train loss = 3.090888557540627\n","iter 3800 train loss = 3.0909464970379754\n","iter 3900 train loss = 3.091253672596624\n","iter 4000 train loss = 3.0925022511431153\n","iter 4100 train loss = 3.092127854893318\n","iter 4200 train loss = 3.0919230765977557\n","iter 4300 train loss = 3.0921207398312673\n","iter 4400 train loss = 3.0915528575970628\n","iter 4500 train loss = 3.0914992152030454\n","iter 4600 train loss = 3.090998279193978\n","iter 4700 train loss = 3.090838697135759\n","iter 4800 train loss = 3.091122049160792\n","iter 4900 train loss = 3.091464208542112\n","Epoch 9 valid loss = 4.128075922123852\n","iter 0 train loss = 3.181232443156023\n","iter 100 train loss = 3.056440427884093\n","iter 200 train loss = 3.0552761023753305\n","iter 300 train loss = 3.0556192217384344\n","iter 400 train loss = 3.053244138703205\n","iter 500 train loss = 3.0498389084924624\n","iter 600 train loss = 3.052819522324115\n","iter 700 train loss = 3.0529526253085906\n","iter 800 train loss = 3.0541350283939335\n","iter 900 train loss = 3.0541265629777863\n","iter 1000 train loss = 3.052176493129597\n","iter 1100 train loss = 3.0552498370291747\n","iter 1200 train loss = 3.056636309301925\n","iter 1300 train loss = 3.0577215946589797\n","iter 1400 train loss = 3.057606535211904\n","iter 1500 train loss = 3.0572049186243118\n","iter 1600 train loss = 3.0567596902834984\n","iter 1700 train loss = 3.0582986234524303\n","iter 1800 train loss = 3.0586950383756712\n","iter 1900 train loss = 3.058746636596052\n","iter 2000 train loss = 3.059262114269186\n","iter 2100 train loss = 3.060633369336351\n","iter 2200 train loss = 3.061808161906631\n","iter 2300 train loss = 3.0621406230599693\n","iter 2400 train loss = 3.0621372367035207\n","iter 2500 train loss = 3.0630132104091183\n","iter 2600 train loss = 3.0638359680025737\n","iter 2700 train loss = 3.063757036564488\n","iter 2800 train loss = 3.0630578674438094\n","iter 2900 train loss = 3.0627703413969405\n","iter 3000 train loss = 3.0624976893956126\n","iter 3100 train loss = 3.063124915916608\n","iter 3200 train loss = 3.0630321413125974\n","iter 3300 train loss = 3.063445100206108\n","iter 3400 train loss = 3.063922891029458\n","iter 3500 train loss = 3.0646323501070927\n","iter 3600 train loss = 3.0647319272382405\n","iter 3700 train loss = 3.0645902467208943\n","iter 3800 train loss = 3.064388590186085\n","iter 3900 train loss = 3.0645538704475674\n","iter 4000 train loss = 3.0639464370999305\n","iter 4100 train loss = 3.064148325904732\n","iter 4200 train loss = 3.064014329933738\n","iter 4300 train loss = 3.0647523529280303\n","iter 4400 train loss = 3.0652605535225375\n","iter 4500 train loss = 3.0651402090335025\n","iter 4600 train loss = 3.065894835548241\n","iter 4700 train loss = 3.066327380095349\n","iter 4800 train loss = 3.066974685337869\n","iter 4900 train loss = 3.066681218105853\n","Epoch 10 valid loss = 4.111141710032619\n","iter 0 train loss = 3.0946289698282876\n","iter 100 train loss = 3.030020864567757\n","iter 200 train loss = 3.0453818710442477\n","iter 300 train loss = 3.029869434472167\n","iter 400 train loss = 3.0306699226275886\n","iter 500 train loss = 3.0332066393096184\n","iter 600 train loss = 3.0301526558213197\n","iter 700 train loss = 3.0301071175739103\n","iter 800 train loss = 3.031614195471998\n","iter 900 train loss = 3.032509612571569\n","iter 1000 train loss = 3.031527098031065\n","iter 1100 train loss = 3.0338414209062283\n","iter 1200 train loss = 3.0342400066006667\n","iter 1300 train loss = 3.03343833245394\n","iter 1400 train loss = 3.035418666322361\n","iter 1500 train loss = 3.0350109297132533\n","iter 1600 train loss = 3.036242591603004\n","iter 1700 train loss = 3.0367706286427505\n","iter 1800 train loss = 3.038021322101828\n","iter 1900 train loss = 3.037872875639582\n","iter 2000 train loss = 3.0398083383473433\n","iter 2100 train loss = 3.0400427322034433\n","iter 2200 train loss = 3.0395940242303214\n","iter 2300 train loss = 3.0400670135768286\n","iter 2400 train loss = 3.040989405172336\n","iter 2500 train loss = 3.041693481733151\n","iter 2600 train loss = 3.041265770762318\n","iter 2700 train loss = 3.0414517294244123\n","iter 2800 train loss = 3.041598932237991\n","iter 2900 train loss = 3.041665330062904\n","iter 3000 train loss = 3.0424297142270227\n","iter 3100 train loss = 3.042633044276042\n","iter 3200 train loss = 3.0429229838351257\n","iter 3300 train loss = 3.0438348890956157\n","iter 3400 train loss = 3.0447362842001713\n","iter 3500 train loss = 3.0456936337012905\n","iter 3600 train loss = 3.0454813479326077\n","iter 3700 train loss = 3.045663258517837\n","iter 3800 train loss = 3.0458911878810744\n","iter 3900 train loss = 3.0459188967250657\n","iter 4000 train loss = 3.046101613956489\n","iter 4100 train loss = 3.046177545692765\n","iter 4200 train loss = 3.046437640103217\n","iter 4300 train loss = 3.0464990509436247\n","iter 4400 train loss = 3.0472275361498387\n","iter 4500 train loss = 3.047956430757276\n","iter 4600 train loss = 3.0480242806412643\n","iter 4700 train loss = 3.048290013273895\n","iter 4800 train loss = 3.048573354571093\n","iter 4900 train loss = 3.0487224560067716\n","Epoch 11 valid loss = 4.127214427139044\n","iter 0 train loss = 3.2708516229059277\n","iter 100 train loss = 3.0441682659339966\n","iter 200 train loss = 3.031423250039606\n","iter 300 train loss = 3.028658715787886\n","iter 400 train loss = 3.0210468149827605\n","iter 500 train loss = 3.023130240786869\n","iter 600 train loss = 3.0206889745545316\n","iter 700 train loss = 3.0228729925265765\n","iter 800 train loss = 3.021924259485805\n","iter 900 train loss = 3.022362265820236\n","iter 1000 train loss = 3.0228835272434393\n","iter 1100 train loss = 3.0226891995800864\n","iter 1200 train loss = 3.0240587404744437\n","iter 1300 train loss = 3.0239064892776373\n","iter 1400 train loss = 3.024991470910082\n","iter 1500 train loss = 3.0235236102498835\n","iter 1600 train loss = 3.025994205116418\n","iter 1700 train loss = 3.027189689572307\n","iter 1800 train loss = 3.0277080187388834\n","iter 1900 train loss = 3.029141583170446\n","iter 2000 train loss = 3.029946042266149\n","iter 2100 train loss = 3.0297066265286774\n","iter 2200 train loss = 3.0300412251492137\n","iter 2300 train loss = 3.029940720104718\n","iter 2400 train loss = 3.030309299719629\n","iter 2500 train loss = 3.0305716366294475\n","iter 2600 train loss = 3.03029020375417\n","iter 2700 train loss = 3.0303416926542934\n","iter 2800 train loss = 3.0305534579417945\n","iter 2900 train loss = 3.031338741806386\n","iter 3000 train loss = 3.0311267991946553\n","iter 3100 train loss = 3.0318500125394428\n","iter 3200 train loss = 3.031725111011836\n","iter 3300 train loss = 3.0323531272064224\n","iter 3400 train loss = 3.032469934173224\n","iter 3500 train loss = 3.032214744481105\n","iter 3600 train loss = 3.0320909322766156\n","iter 3700 train loss = 3.0319753182082345\n","iter 3800 train loss = 3.0326471190028585\n","iter 3900 train loss = 3.0320054820501174\n","iter 4000 train loss = 3.031646240188888\n","iter 4100 train loss = 3.0320074465333464\n","iter 4200 train loss = 3.031639025979825\n","iter 4300 train loss = 3.0316507519584803\n","iter 4400 train loss = 3.032217002921264\n","iter 4500 train loss = 3.032735802590272\n","iter 4600 train loss = 3.0333248575223437\n","iter 4700 train loss = 3.033625897060612\n","iter 4800 train loss = 3.034005553934234\n","iter 4900 train loss = 3.0341522012169246\n","Epoch 12 valid loss = 4.137587785161363\n","iter 0 train loss = 2.857062548722207\n","iter 100 train loss = 3.003024802867166\n","iter 200 train loss = 3.019377375966614\n","iter 300 train loss = 3.018726481454462\n","iter 400 train loss = 3.0140390895562215\n","iter 500 train loss = 3.0119212702069995\n","iter 600 train loss = 3.01224590480127\n","iter 700 train loss = 3.0100316553781967\n","iter 800 train loss = 3.0088062458657574\n","iter 900 train loss = 3.0104342297034212\n","iter 1000 train loss = 3.006858734970801\n","iter 1100 train loss = 3.0065722248865048\n","iter 1200 train loss = 3.0069281369847882\n","iter 1300 train loss = 3.007115911207302\n","iter 1400 train loss = 3.007347515077525\n","iter 1500 train loss = 3.0078046837113455\n","iter 1600 train loss = 3.0092526682243728\n","iter 1700 train loss = 3.0103574585416686\n","iter 1800 train loss = 3.01097273518261\n","iter 1900 train loss = 3.0113825397162044\n","iter 2000 train loss = 3.012183147204625\n","iter 2100 train loss = 3.0133384415727322\n","iter 2200 train loss = 3.0133266247339168\n","iter 2300 train loss = 3.012588084853716\n","iter 2400 train loss = 3.012345565052392\n","iter 2500 train loss = 3.0122584014038325\n","iter 2600 train loss = 3.0125441050674473\n","iter 2700 train loss = 3.012704443610287\n","iter 2800 train loss = 3.0119343798582685\n","iter 2900 train loss = 3.012737420630666\n","iter 3000 train loss = 3.012969541910165\n","iter 3100 train loss = 3.0133868821001513\n","iter 3200 train loss = 3.0137412603733913\n","iter 3300 train loss = 3.014350935838655\n","iter 3400 train loss = 3.0157459393393156\n","iter 3500 train loss = 3.015903830738935\n","iter 3600 train loss = 3.0164557170318465\n","iter 3700 train loss = 3.0161764043615036\n","iter 3800 train loss = 3.016311963655285\n","iter 3900 train loss = 3.016621571204574\n","iter 4000 train loss = 3.0165209383614893\n","iter 4100 train loss = 3.0163283254223168\n","iter 4200 train loss = 3.016369488972999\n","iter 4300 train loss = 3.0170553934527544\n","iter 4400 train loss = 3.0177723435102988\n","iter 4500 train loss = 3.018603691114027\n","iter 4600 train loss = 3.0192429368990994\n","iter 4700 train loss = 3.0198688140460557\n","iter 4800 train loss = 3.020279848452332\n","iter 4900 train loss = 3.020901484826526\n","Epoch 13 valid loss = 4.128480509219155\n","iter 0 train loss = 2.7825350785136815\n","iter 100 train loss = 2.9829115996200546\n","iter 200 train loss = 2.98662576226251\n","iter 300 train loss = 2.990972273646857\n","iter 400 train loss = 2.9924124917867947\n","iter 500 train loss = 2.9994949615970983\n","iter 600 train loss = 3.000901654543307\n","iter 700 train loss = 3.0004910247037677\n","iter 800 train loss = 3.0029307252534556\n","iter 900 train loss = 3.0028963765736023\n","iter 1000 train loss = 3.0034722356521626\n","iter 1100 train loss = 3.00266297897993\n","iter 1200 train loss = 3.0020082519324647\n","iter 1300 train loss = 3.000915818703443\n","iter 1400 train loss = 2.999700133725754\n","iter 1500 train loss = 3.0005153067099726\n","iter 1600 train loss = 3.000284952684195\n","iter 1700 train loss = 2.9999807272501053\n","iter 1800 train loss = 3.0003410316659824\n","iter 1900 train loss = 3.001084204495969\n","iter 2000 train loss = 3.0020631079202134\n","iter 2100 train loss = 3.0017118387023243\n","iter 2200 train loss = 3.0023273319909283\n","iter 2300 train loss = 3.0032010474175377\n","iter 2400 train loss = 3.002735382834217\n","iter 2500 train loss = 3.002617264207618\n","iter 2600 train loss = 3.004313467771645\n","iter 2700 train loss = 3.0043149383558907\n","iter 2800 train loss = 3.00394974704827\n","iter 2900 train loss = 3.0051473502427575\n","iter 3000 train loss = 3.0054683022249926\n","iter 3100 train loss = 3.0061733627284353\n","iter 3200 train loss = 3.0059003241028392\n","iter 3300 train loss = 3.005715699909516\n","iter 3400 train loss = 3.006498494537653\n","iter 3500 train loss = 3.00638889277723\n","iter 3600 train loss = 3.0065775610300056\n","iter 3700 train loss = 3.0066411387029137\n","iter 3800 train loss = 3.0070070167674543\n","iter 3900 train loss = 3.0071045928396853\n","iter 4000 train loss = 3.0068045324961927\n","iter 4100 train loss = 3.007237326694522\n","iter 4200 train loss = 3.00709388156728\n","iter 4300 train loss = 3.0076666307343594\n","iter 4400 train loss = 3.0081638556421253\n","iter 4500 train loss = 3.0081399886873395\n","iter 4600 train loss = 3.00816874978184\n","iter 4700 train loss = 3.008686373591293\n","iter 4800 train loss = 3.009147730480939\n","iter 4900 train loss = 3.0098248330653483\n","Epoch 14 valid loss = 4.1564823827907205\n","iter 0 train loss = 3.172558862036401\n","iter 100 train loss = 3.003578792869673\n","iter 200 train loss = 2.994654434414022\n","iter 300 train loss = 2.994485474573247\n","iter 400 train loss = 2.9899976181704058\n","iter 500 train loss = 2.9889760817720643\n","iter 600 train loss = 2.9852296284715356\n","iter 700 train loss = 2.9830103137244652\n","iter 800 train loss = 2.9819720570681794\n","iter 900 train loss = 2.9837480434704475\n","iter 1000 train loss = 2.9827551809027426\n","iter 1100 train loss = 2.983821885367572\n","iter 1200 train loss = 2.982873163876036\n","iter 1300 train loss = 2.9833123042660836\n","iter 1400 train loss = 2.983394227107741\n","iter 1500 train loss = 2.9836628407048558\n","iter 1600 train loss = 2.9852271485729887\n","iter 1700 train loss = 2.984534701295543\n","iter 1800 train loss = 2.9859751364759872\n","iter 1900 train loss = 2.9869644415455228\n","iter 2000 train loss = 2.988041583980441\n","iter 2100 train loss = 2.9880721428633366\n","iter 2200 train loss = 2.989903940367943\n","iter 2300 train loss = 2.989542259429728\n","iter 2400 train loss = 2.9901223572045317\n","iter 2500 train loss = 2.9906347135160485\n","iter 2600 train loss = 2.9909373721579517\n","iter 2700 train loss = 2.991119149411552\n","iter 2800 train loss = 2.9929040351079097\n","iter 2900 train loss = 2.992555633450133\n","iter 3000 train loss = 2.992718460967019\n","iter 3100 train loss = 2.993106135280009\n","iter 3200 train loss = 2.993765617572262\n","iter 3300 train loss = 2.994510398254924\n","iter 3400 train loss = 2.9943155304534343\n","iter 3500 train loss = 2.995634463926521\n","iter 3600 train loss = 2.9961161976332296\n","iter 3700 train loss = 2.995932627138781\n","iter 3800 train loss = 2.996392292268814\n","iter 3900 train loss = 2.9971600293300016\n","iter 4000 train loss = 2.997577303529939\n","iter 4100 train loss = 2.998102576108437\n","iter 4200 train loss = 2.99805811764828\n","iter 4300 train loss = 2.9978605217474175\n","iter 4400 train loss = 2.998265385424797\n","iter 4500 train loss = 2.9982672224979763\n","iter 4600 train loss = 2.9986037568792097\n","iter 4700 train loss = 2.998788519840339\n","iter 4800 train loss = 2.9984549134758103\n","iter 4900 train loss = 2.998709186954668\n","Epoch 15 valid loss = 4.146158577586011\n","iter 0 train loss = 2.9655320561152463\n","iter 100 train loss = 2.963715283281921\n","iter 200 train loss = 2.9597306237766476\n","iter 300 train loss = 2.9656922118460116\n","iter 400 train loss = 2.9710513270961023\n","iter 500 train loss = 2.9683310095878377\n","iter 600 train loss = 2.9715639497957564\n","iter 700 train loss = 2.9695839740695096\n","iter 800 train loss = 2.9696905259933146\n","iter 900 train loss = 2.970849924538859\n","iter 1000 train loss = 2.973275002823125\n","iter 1100 train loss = 2.973439271798311\n","iter 1200 train loss = 2.9747488015676558\n","iter 1300 train loss = 2.977767453033064\n","iter 1400 train loss = 2.978187988467218\n","iter 1500 train loss = 2.977563127969104\n","iter 1600 train loss = 2.977930644924589\n","iter 1700 train loss = 2.9779175373028037\n","iter 1800 train loss = 2.9783044185469847\n","iter 1900 train loss = 2.9781644213606935\n","iter 2000 train loss = 2.97938557379796\n","iter 2100 train loss = 2.9790151471539463\n","iter 2200 train loss = 2.9796544810765284\n","iter 2300 train loss = 2.980698146014011\n","iter 2400 train loss = 2.981572345507825\n","iter 2500 train loss = 2.983460591998791\n","iter 2600 train loss = 2.9829282320210853\n","iter 2700 train loss = 2.9829139275086667\n","iter 2800 train loss = 2.9829436067462467\n","iter 2900 train loss = 2.9828272550323707\n","iter 3000 train loss = 2.9831225884603976\n","iter 3100 train loss = 2.9830221513412987\n","iter 3200 train loss = 2.983146530415319\n","iter 3300 train loss = 2.984523242943787\n","iter 3400 train loss = 2.98459163842169\n","iter 3500 train loss = 2.9852080846489333\n","iter 3600 train loss = 2.986325550378063\n","iter 3700 train loss = 2.986558447623636\n","iter 3800 train loss = 2.9869965212668688\n","iter 3900 train loss = 2.9868987535080813\n","iter 4000 train loss = 2.9867603880338955\n","iter 4100 train loss = 2.987018148344164\n","iter 4200 train loss = 2.986890758694767\n","iter 4300 train loss = 2.9872143161778\n","iter 4400 train loss = 2.987449327494479\n","iter 4500 train loss = 2.9877559309900397\n","iter 4600 train loss = 2.98809901689596\n","iter 4700 train loss = 2.988832105062303\n","iter 4800 train loss = 2.9887164171100387\n","iter 4900 train loss = 2.9886674178154227\n","Epoch 16 valid loss = 4.151455268575046\n","iter 0 train loss = 3.1207019158129423\n","iter 100 train loss = 2.9574804808130253\n","iter 200 train loss = 2.9514736150467407\n","iter 300 train loss = 2.950498410378972\n","iter 400 train loss = 2.9587953061698635\n","iter 500 train loss = 2.9563828867084445\n","iter 600 train loss = 2.9593618810303672\n","iter 700 train loss = 2.9566336913733076\n","iter 800 train loss = 2.955875769415747\n","iter 900 train loss = 2.953944187489022\n","iter 1000 train loss = 2.9552689358819078\n","iter 1100 train loss = 2.9547253369856676\n","iter 1200 train loss = 2.9561444595602113\n","iter 1300 train loss = 2.957736319479958\n","iter 1400 train loss = 2.9589063426721007\n","iter 1500 train loss = 2.960468141393674\n","iter 1600 train loss = 2.96137159502335\n","iter 1700 train loss = 2.9613390065077105\n","iter 1800 train loss = 2.960613571430463\n","iter 1900 train loss = 2.9605796504048936\n","iter 2000 train loss = 2.9604929806418845\n","iter 2100 train loss = 2.9604449522495377\n","iter 2200 train loss = 2.9613838241161328\n","iter 2300 train loss = 2.9608206087860562\n","iter 2400 train loss = 2.961144263199566\n","iter 2500 train loss = 2.9607671126745188\n","iter 2600 train loss = 2.9603939427400587\n","iter 2700 train loss = 2.9604968725673086\n","iter 2800 train loss = 2.9609452587623926\n","iter 2900 train loss = 2.960526481059265\n","iter 3000 train loss = 2.96047780804332\n","iter 3100 train loss = 2.9604508599152157\n","iter 3200 train loss = 2.9603426089778377\n","iter 3300 train loss = 2.960857937264323\n","iter 3400 train loss = 2.96102232103953\n","iter 3500 train loss = 2.9616044968111637\n","iter 3600 train loss = 2.961850360622665\n","iter 3700 train loss = 2.961955153021326\n","iter 3800 train loss = 2.9616684362603443\n","iter 3900 train loss = 2.961877977230144\n","iter 4000 train loss = 2.9616365173095267\n","iter 4100 train loss = 2.961085169435559\n","iter 4200 train loss = 2.960560281571371\n","iter 4300 train loss = 2.960315787811013\n","iter 4400 train loss = 2.9598993418565525\n","iter 4500 train loss = 2.959906439431189\n","iter 4600 train loss = 2.960077961896271\n","iter 4700 train loss = 2.959870876058586\n","iter 4800 train loss = 2.9600713717813893\n","iter 4900 train loss = 2.9603306986097766\n","Epoch 17 valid loss = 4.14738681792843\n","iter 0 train loss = 2.763509330445955\n","iter 100 train loss = 2.960766443155429\n","iter 200 train loss = 2.9625624282943215\n","iter 300 train loss = 2.9609442282545895\n","iter 400 train loss = 2.9542142844824144\n","iter 500 train loss = 2.9552253902305208\n","iter 600 train loss = 2.9582252135450218\n","iter 700 train loss = 2.9574025246691646\n","iter 800 train loss = 2.9566736830199027\n","iter 900 train loss = 2.9553584315107324\n","iter 1000 train loss = 2.9562952300165537\n","iter 1100 train loss = 2.9544549695633258\n","iter 1200 train loss = 2.954246844550325\n","iter 1300 train loss = 2.955818708817714\n","iter 1400 train loss = 2.9576833429875484\n","iter 1500 train loss = 2.955883839253062\n","iter 1600 train loss = 2.956272047834607\n","iter 1700 train loss = 2.955923351493049\n","iter 1800 train loss = 2.955728777834852\n","iter 1900 train loss = 2.9551415402921606\n","iter 2000 train loss = 2.9548383470525836\n","iter 2100 train loss = 2.953718500259748\n","iter 2200 train loss = 2.9533515391863796\n","iter 2300 train loss = 2.9525541691766617\n","iter 2400 train loss = 2.953275269295427\n","iter 2500 train loss = 2.952094351490303\n","iter 2600 train loss = 2.9520055714852873\n","iter 2700 train loss = 2.9530296860204777\n","iter 2800 train loss = 2.953403932362411\n","iter 2900 train loss = 2.954449720767936\n","iter 3000 train loss = 2.9539791775136353\n","iter 3100 train loss = 2.9547376721474214\n","iter 3200 train loss = 2.955091755452103\n","iter 3300 train loss = 2.9551777008675653\n","iter 3400 train loss = 2.95542939515156\n","iter 3500 train loss = 2.9559484586517564\n","iter 3600 train loss = 2.9562020435783474\n","iter 3700 train loss = 2.956684091247798\n","iter 3800 train loss = 2.9565694749466327\n","iter 3900 train loss = 2.9563971122148707\n","iter 4000 train loss = 2.9563203717758015\n","iter 4100 train loss = 2.956364686589685\n","iter 4200 train loss = 2.9566448942075176\n","iter 4300 train loss = 2.9560160992245357\n","iter 4400 train loss = 2.9555964906383636\n","iter 4500 train loss = 2.95592748739404\n","iter 4600 train loss = 2.9557783095839283\n","iter 4700 train loss = 2.9563712750601217\n","iter 4800 train loss = 2.9565534325854235\n","iter 4900 train loss = 2.956982892364925\n","Epoch 18 valid loss = 4.146225921621982\n","iter 0 train loss = 2.729855514886811\n","iter 100 train loss = 2.939938953608509\n","iter 200 train loss = 2.9518599443715394\n","iter 300 train loss = 2.9542097676062964\n","iter 400 train loss = 2.949802959376181\n","iter 500 train loss = 2.950569177995159\n","iter 600 train loss = 2.9524115911784317\n","iter 700 train loss = 2.952928077186759\n","iter 800 train loss = 2.9518644079295733\n","iter 900 train loss = 2.95063458471399\n","iter 1000 train loss = 2.9513920664053432\n","iter 1100 train loss = 2.9497308024025166\n","iter 1200 train loss = 2.950827959216396\n","iter 1300 train loss = 2.9516840479068933\n","iter 1400 train loss = 2.9527849118785414\n","iter 1500 train loss = 2.953193634803446\n","iter 1600 train loss = 2.952828037883883\n","iter 1700 train loss = 2.954336156614644\n","iter 1800 train loss = 2.955231829360217\n","iter 1900 train loss = 2.9553421846656853\n","iter 2000 train loss = 2.9567685391758713\n","iter 2100 train loss = 2.9571461268548673\n","iter 2200 train loss = 2.956509958351135\n","iter 2300 train loss = 2.9559912650238886\n","iter 2400 train loss = 2.956474320609522\n","iter 2500 train loss = 2.957151761699278\n","iter 2600 train loss = 2.956900958980082\n","iter 2700 train loss = 2.9573977000343157\n","iter 2800 train loss = 2.956621319025018\n","iter 2900 train loss = 2.95577803599966\n","iter 3000 train loss = 2.9558342062814233\n","iter 3100 train loss = 2.95621045912135\n","iter 3200 train loss = 2.956317236526019\n","iter 3300 train loss = 2.9552931251124335\n","iter 3400 train loss = 2.9554895019899963\n","iter 3500 train loss = 2.9549742236883225\n","iter 3600 train loss = 2.95496487523168\n","iter 3700 train loss = 2.9552000770154723\n","iter 3800 train loss = 2.9550288042668758\n","iter 3900 train loss = 2.955350344220866\n","iter 4000 train loss = 2.955101072887421\n","iter 4100 train loss = 2.9551981575552615\n","iter 4200 train loss = 2.954996726658069\n","iter 4300 train loss = 2.955389569771026\n","iter 4400 train loss = 2.9552846548581058\n","iter 4500 train loss = 2.955971215483113\n","iter 4600 train loss = 2.9559868342049618\n","iter 4700 train loss = 2.9560708997273437\n","iter 4800 train loss = 2.955583786387335\n","iter 4900 train loss = 2.955750377512282\n","Epoch 19 valid loss = 4.1446570136111776\n","iter 0 train loss = 2.8524304406117587\n","iter 100 train loss = 2.956096268909377\n","iter 200 train loss = 2.950180381158971\n","iter 300 train loss = 2.9448814752115613\n","iter 400 train loss = 2.9385492838321445\n","iter 500 train loss = 2.9451118785136425\n","iter 600 train loss = 2.9430743007530813\n","iter 700 train loss = 2.945392132790796\n","iter 800 train loss = 2.9445922699315883\n","iter 900 train loss = 2.941218327772143\n","iter 1000 train loss = 2.9437207583327525\n","iter 1100 train loss = 2.943358295742748\n","iter 1200 train loss = 2.944740924375773\n","iter 1300 train loss = 2.9451552418762614\n","iter 1400 train loss = 2.9469020215392403\n","iter 1500 train loss = 2.9475489819905674\n","iter 1600 train loss = 2.9477124672757014\n","iter 1700 train loss = 2.9492116350002013\n","iter 1800 train loss = 2.9502543597154216\n","iter 1900 train loss = 2.9505263636105465\n","iter 2000 train loss = 2.950810001816521\n","iter 2100 train loss = 2.9514215909658175\n","iter 2200 train loss = 2.9510417120926262\n","iter 2300 train loss = 2.950988503473074\n","iter 2400 train loss = 2.951035983377526\n","iter 2500 train loss = 2.951230959574262\n","iter 2600 train loss = 2.9520036512236056\n","iter 2700 train loss = 2.9522283228996766\n","iter 2800 train loss = 2.952843539948555\n","iter 2900 train loss = 2.9523791327083244\n","iter 3000 train loss = 2.9524730013945217\n","iter 3100 train loss = 2.9531947293514085\n","iter 3200 train loss = 2.9533188489162985\n","iter 3300 train loss = 2.9535137423631603\n","iter 3400 train loss = 2.9544732815335255\n","iter 3500 train loss = 2.9544860523806804\n","iter 3600 train loss = 2.953842435950348\n","iter 3700 train loss = 2.9538138640727496\n","iter 3800 train loss = 2.953819404967488\n","iter 3900 train loss = 2.9535481404541666\n","iter 4000 train loss = 2.9540781339056865\n","iter 4100 train loss = 2.953851969506535\n","iter 4200 train loss = 2.954135524736386\n","iter 4300 train loss = 2.9546318496934645\n","iter 4400 train loss = 2.954497127193392\n","iter 4500 train loss = 2.9542547549958615\n","iter 4600 train loss = 2.954519027347564\n","iter 4700 train loss = 2.954536806183631\n","iter 4800 train loss = 2.9543528870738087\n","iter 4900 train loss = 2.9544908123917604\n","Epoch 20 valid loss = 4.145415037050287\n","iter 0 train loss = 3.2152860419694766\n","iter 100 train loss = 2.950304431629675\n","iter 200 train loss = 2.9441718948790165\n","iter 300 train loss = 2.937664455918666\n","iter 400 train loss = 2.9418765907943136\n","iter 500 train loss = 2.9386419879740675\n","iter 600 train loss = 2.938769495635915\n","iter 700 train loss = 2.9433559797229765\n","iter 800 train loss = 2.9436115502567572\n","iter 900 train loss = 2.9408734390773854\n","iter 1000 train loss = 2.9408544990928522\n","iter 1100 train loss = 2.940916358737569\n","iter 1200 train loss = 2.9425878482984644\n","iter 1300 train loss = 2.9464561961059803\n","iter 1400 train loss = 2.945337716637033\n","iter 1500 train loss = 2.945861012066228\n","iter 1600 train loss = 2.9467020897743796\n","iter 1700 train loss = 2.946907436359745\n","iter 1800 train loss = 2.9466370029967868\n","iter 1900 train loss = 2.9460096064372423\n","iter 2000 train loss = 2.9463261111827768\n","iter 2100 train loss = 2.9475842450149043\n","iter 2200 train loss = 2.948265045875566\n","iter 2300 train loss = 2.947686382376632\n","iter 2400 train loss = 2.947022838081712\n","iter 2500 train loss = 2.947251173938464\n","iter 2600 train loss = 2.948572460502028\n","iter 2700 train loss = 2.9488464335469096\n","iter 2800 train loss = 2.9491725613486324\n","iter 2900 train loss = 2.9488656673584304\n","iter 3000 train loss = 2.9491050236863954\n","iter 3100 train loss = 2.948869600029985\n","iter 3200 train loss = 2.9489857101011117\n","iter 3300 train loss = 2.949594253770824\n","iter 3400 train loss = 2.9502597238915422\n","iter 3500 train loss = 2.950031973367891\n","iter 3600 train loss = 2.950565551044518\n","iter 3700 train loss = 2.9507288592794283\n","iter 3800 train loss = 2.951181261739921\n","iter 3900 train loss = 2.95142880966512\n","iter 4000 train loss = 2.9514200069446797\n","iter 4100 train loss = 2.951654918407681\n","iter 4200 train loss = 2.9515784997157946\n","iter 4300 train loss = 2.951618924688539\n","iter 4400 train loss = 2.9512280670143243\n","iter 4500 train loss = 2.951553669115449\n","iter 4600 train loss = 2.9510362934237717\n","iter 4700 train loss = 2.951050267479206\n","iter 4800 train loss = 2.9515839367670025\n","iter 4900 train loss = 2.9520972568893815\n","Epoch 21 valid loss = 4.146323516321936\n","iter 0 train loss = 3.099154165662858\n","iter 100 train loss = 2.9256231406110222\n","iter 200 train loss = 2.939015560879848\n","iter 300 train loss = 2.946596338430302\n","iter 400 train loss = 2.9464100739703607\n","iter 500 train loss = 2.949619340590912\n","iter 600 train loss = 2.9518076288809127\n","iter 700 train loss = 2.9459416781327574\n","iter 800 train loss = 2.948387196348733\n","iter 900 train loss = 2.9468636796168024\n","iter 1000 train loss = 2.9466610362883774\n","iter 1100 train loss = 2.9509690264633575\n","iter 1200 train loss = 2.950578601455575\n","iter 1300 train loss = 2.9497053678919265\n","iter 1400 train loss = 2.9516364888576443\n","iter 1500 train loss = 2.9532447013627383\n","iter 1600 train loss = 2.9515095037920718\n","iter 1700 train loss = 2.950074151820303\n","iter 1800 train loss = 2.9501143404901065\n","iter 1900 train loss = 2.950615145651499\n","iter 2000 train loss = 2.950557937403443\n","iter 2100 train loss = 2.950397014200846\n","iter 2200 train loss = 2.950514037043342\n","iter 2300 train loss = 2.951162396617797\n","iter 2400 train loss = 2.9502183107372497\n","iter 2500 train loss = 2.9501742173487235\n","iter 2600 train loss = 2.950755833507775\n","iter 2700 train loss = 2.950528183329041\n","iter 2800 train loss = 2.9502235218284176\n","iter 2900 train loss = 2.950036370415208\n","iter 3000 train loss = 2.9493679251221727\n","iter 3100 train loss = 2.949411979579949\n","iter 3200 train loss = 2.9491988007094108\n","iter 3300 train loss = 2.949192861869543\n","iter 3400 train loss = 2.949354095567644\n","iter 3500 train loss = 2.9494001422812772\n","iter 3600 train loss = 2.9492684928390673\n","iter 3700 train loss = 2.948926790109383\n","iter 3800 train loss = 2.948778329030804\n","iter 3900 train loss = 2.9486216077585468\n","iter 4000 train loss = 2.948907833546607\n","iter 4100 train loss = 2.949561925167171\n","iter 4200 train loss = 2.949316550315557\n","iter 4300 train loss = 2.949180320447125\n","iter 4400 train loss = 2.9493711601203203\n","iter 4500 train loss = 2.950075312182847\n","iter 4600 train loss = 2.949989980506787\n","iter 4700 train loss = 2.9497754004435413\n","iter 4800 train loss = 2.9505170242515266\n","iter 4900 train loss = 2.950413050800623\n","Epoch 22 valid loss = 4.144564488208526\n","iter 0 train loss = 3.0571109952560667\n","iter 100 train loss = 2.944821973712738\n","iter 200 train loss = 2.949425347896338\n","iter 300 train loss = 2.9509913763304008\n","iter 400 train loss = 2.949681739121424\n","iter 500 train loss = 2.950012156581983\n","iter 600 train loss = 2.947776290569551\n","iter 700 train loss = 2.9463059135925076\n","iter 800 train loss = 2.944269052665225\n","iter 900 train loss = 2.9439528318586\n","iter 1000 train loss = 2.9429147947771686\n","iter 1100 train loss = 2.9431365162385883\n","iter 1200 train loss = 2.9435778813259303\n","iter 1300 train loss = 2.9471135217251025\n","iter 1400 train loss = 2.9475857171787343\n","iter 1500 train loss = 2.9481260453195457\n","iter 1600 train loss = 2.947226709185992\n","iter 1700 train loss = 2.947003846332813\n","iter 1800 train loss = 2.9471894591982015\n","iter 1900 train loss = 2.946127271749949\n","iter 2000 train loss = 2.945394143944901\n","iter 2100 train loss = 2.9455915924458353\n","iter 2200 train loss = 2.9452871664032396\n","iter 2300 train loss = 2.946089199957341\n","iter 2400 train loss = 2.946227585271896\n","iter 2500 train loss = 2.9460418515062408\n","iter 2600 train loss = 2.9460488843845583\n","iter 2700 train loss = 2.945737152459641\n","iter 2800 train loss = 2.945741764216876\n","iter 2900 train loss = 2.9455867514007448\n","iter 3000 train loss = 2.9457768960079553\n","iter 3100 train loss = 2.9448588785124943\n","iter 3200 train loss = 2.945309381606436\n","iter 3300 train loss = 2.9454926120232887\n","iter 3400 train loss = 2.9452773271697414\n","iter 3500 train loss = 2.9450163470294526\n","iter 3600 train loss = 2.9450647258107194\n","iter 3700 train loss = 2.94462528028795\n","iter 3800 train loss = 2.944164223881047\n","iter 3900 train loss = 2.944109251868352\n","iter 4000 train loss = 2.9448095509823387\n","iter 4100 train loss = 2.9448453338915117\n","iter 4200 train loss = 2.9457899682023716\n","iter 4300 train loss = 2.945801131430231\n","iter 4400 train loss = 2.9458418655064085\n","iter 4500 train loss = 2.946043979289643\n","iter 4600 train loss = 2.946020486149561\n","iter 4700 train loss = 2.9460505058653337\n","iter 4800 train loss = 2.9468382419785533\n","iter 4900 train loss = 2.9469939351244125\n","Epoch 23 valid loss = 4.144764917647932\n","iter 0 train loss = 3.111025453901188\n","iter 100 train loss = 2.945794119723379\n","iter 200 train loss = 2.924800295354177\n","iter 300 train loss = 2.93908978885253\n","iter 400 train loss = 2.936906747981471\n","iter 500 train loss = 2.94118636653105\n","iter 600 train loss = 2.938464144016211\n","iter 700 train loss = 2.9383509287228993\n","iter 800 train loss = 2.9374127463258226\n","iter 900 train loss = 2.939229096246897\n","iter 1000 train loss = 2.9375857364354663\n","iter 1100 train loss = 2.937773740788741\n","iter 1200 train loss = 2.938477747688824\n","iter 1300 train loss = 2.9401374115619676\n","iter 1400 train loss = 2.9420878527284056\n","iter 1500 train loss = 2.941266567700879\n","iter 1600 train loss = 2.9407513139777945\n","iter 1700 train loss = 2.9415772678675896\n","iter 1800 train loss = 2.943726461414425\n","iter 1900 train loss = 2.943965377718112\n","iter 2000 train loss = 2.9443269220985226\n","iter 2100 train loss = 2.9454543566649574\n","iter 2200 train loss = 2.945548409455233\n","iter 2300 train loss = 2.946024355262896\n","iter 2400 train loss = 2.946555574798963\n","iter 2500 train loss = 2.9474357963045206\n","iter 2600 train loss = 2.9477080966768066\n","iter 2700 train loss = 2.946368662932439\n","iter 2800 train loss = 2.9467821121150886\n","iter 2900 train loss = 2.945892841825038\n","iter 3000 train loss = 2.945548844676269\n","iter 3100 train loss = 2.945110472809927\n","iter 3200 train loss = 2.9445927140381727\n","iter 3300 train loss = 2.944132092477953\n","iter 3400 train loss = 2.9444834311576886\n","iter 3500 train loss = 2.944707625262186\n","iter 3600 train loss = 2.9452223949371628\n","iter 3700 train loss = 2.9460859175484564\n","iter 3800 train loss = 2.946045785066406\n","iter 3900 train loss = 2.94539477737189\n","iter 4000 train loss = 2.946223716326247\n","iter 4100 train loss = 2.9465204046630524\n","iter 4200 train loss = 2.946917113481212\n","iter 4300 train loss = 2.9471664705385665\n","iter 4400 train loss = 2.9475386335680396\n","iter 4500 train loss = 2.947690081150899\n","iter 4600 train loss = 2.947168714158988\n","iter 4700 train loss = 2.9476618875365785\n","iter 4800 train loss = 2.947891541093679\n","iter 4900 train loss = 2.9474310512338397\n","Epoch 24 valid loss = 4.144325850690288\n","iter 0 train loss = 2.8407534644717263\n","iter 100 train loss = 2.9529791183164553\n","iter 200 train loss = 2.9426183449412253\n","iter 300 train loss = 2.935864322500179\n","iter 400 train loss = 2.9407404281434886\n","iter 500 train loss = 2.9420869402384455\n","iter 600 train loss = 2.9457436770876733\n","iter 700 train loss = 2.9479844835069446\n","iter 800 train loss = 2.950505510906631\n","iter 900 train loss = 2.9492243324857776\n","iter 1000 train loss = 2.9472245805648387\n","iter 1100 train loss = 2.94667795764114\n","iter 1200 train loss = 2.9451557091479503\n","iter 1300 train loss = 2.9439287377129353\n","iter 1400 train loss = 2.942706209941975\n","iter 1500 train loss = 2.943908442571818\n","iter 1600 train loss = 2.9441989184893145\n","iter 1700 train loss = 2.9455844675386644\n","iter 1800 train loss = 2.945324212891852\n","iter 1900 train loss = 2.945645236898477\n","iter 2000 train loss = 2.9461631483063995\n","iter 2100 train loss = 2.9465186829818726\n","iter 2200 train loss = 2.9473975656604465\n","iter 2300 train loss = 2.946496206183346\n","iter 2400 train loss = 2.9464871387002636\n","iter 2500 train loss = 2.94730843516262\n","iter 2600 train loss = 2.947543322613089\n","iter 2700 train loss = 2.9477965320649844\n","iter 2800 train loss = 2.948102326938577\n","iter 2900 train loss = 2.948019587639709\n","iter 3000 train loss = 2.9482145726585545\n","iter 3100 train loss = 2.948428081180316\n","iter 3200 train loss = 2.9475724626710376\n","iter 3300 train loss = 2.9477072076222313\n","iter 3400 train loss = 2.9470232304141684\n","iter 3500 train loss = 2.9470585657426778\n","iter 3600 train loss = 2.9475710530409636\n","iter 3700 train loss = 2.947336093947314\n","iter 3800 train loss = 2.9468825576491025\n","iter 3900 train loss = 2.946982294014474\n","iter 4000 train loss = 2.9470553958115295\n","iter 4100 train loss = 2.9476585554753783\n","iter 4200 train loss = 2.9475549236375413\n","iter 4300 train loss = 2.9474209119276855\n","iter 4400 train loss = 2.947155310815735\n","iter 4500 train loss = 2.9466664946607044\n","iter 4600 train loss = 2.9468330760826658\n","iter 4700 train loss = 2.9468871130081604\n","iter 4800 train loss = 2.946961208299993\n","iter 4900 train loss = 2.9471348784232227\n","Epoch 25 valid loss = 4.144391732024925\n","iter 0 train loss = 3.0175660279420047\n","iter 100 train loss = 2.9474362408819554\n","iter 200 train loss = 2.956167314309445\n","iter 300 train loss = 2.9617235318227193\n","iter 400 train loss = 2.9550805818712633\n","iter 500 train loss = 2.9485714206654574\n","iter 600 train loss = 2.949979854068738\n","iter 700 train loss = 2.9509103305102466\n","iter 800 train loss = 2.950826871152902\n","iter 900 train loss = 2.9519092401770335\n","iter 1000 train loss = 2.9491960303784506\n","iter 1100 train loss = 2.947547765983971\n","iter 1200 train loss = 2.9494361690192052\n","iter 1300 train loss = 2.949658405147453\n","iter 1400 train loss = 2.9491300715857873\n","iter 1500 train loss = 2.9477764424780752\n","iter 1600 train loss = 2.94825900739885\n","iter 1700 train loss = 2.9470580152301444\n","iter 1800 train loss = 2.9453243763238417\n","iter 1900 train loss = 2.9454889418190247\n","iter 2000 train loss = 2.944298176178539\n","iter 2100 train loss = 2.944962598597033\n","iter 2200 train loss = 2.9456648794866216\n","iter 2300 train loss = 2.945870635929133\n","iter 2400 train loss = 2.946787585833091\n","iter 2500 train loss = 2.946316663270051\n","iter 2600 train loss = 2.9457515285202094\n","iter 2700 train loss = 2.945647751167297\n","iter 2800 train loss = 2.9458678286426134\n","iter 2900 train loss = 2.945956281657107\n","iter 3000 train loss = 2.9456435836430854\n","iter 3100 train loss = 2.9461995192202495\n","iter 3200 train loss = 2.9458845699993974\n","iter 3300 train loss = 2.945786377478575\n","iter 3400 train loss = 2.945805962684133\n","iter 3500 train loss = 2.9461626770688496\n","iter 3600 train loss = 2.9456287223254307\n","iter 3700 train loss = 2.9460732002825054\n","iter 3800 train loss = 2.946110195034411\n","iter 3900 train loss = 2.9464169303567043\n","iter 4000 train loss = 2.946185687452175\n","iter 4100 train loss = 2.9465536073734633\n","iter 4200 train loss = 2.946934392306006\n","iter 4300 train loss = 2.9468990910282367\n","iter 4400 train loss = 2.9472001938176167\n","iter 4500 train loss = 2.9471781618784156\n","iter 4600 train loss = 2.947080582185516\n","iter 4700 train loss = 2.947032615432374\n","iter 4800 train loss = 2.946949133674024\n","iter 4900 train loss = 2.94698181942369\n","Epoch 26 valid loss = 4.144395253989579\n","iter 0 train loss = 2.6566478128965736\n","iter 100 train loss = 2.9292354777561886\n","iter 200 train loss = 2.931107425098591\n","iter 300 train loss = 2.9327641061458647\n","iter 400 train loss = 2.9425555569127457\n","iter 500 train loss = 2.9463123761784824\n","iter 600 train loss = 2.9460822316649584\n","iter 700 train loss = 2.9461824583634493\n","iter 800 train loss = 2.9467044854162987\n","iter 900 train loss = 2.9488435241960684\n","iter 1000 train loss = 2.948680667864896\n","iter 1100 train loss = 2.9487819299872835\n","iter 1200 train loss = 2.9479609301024614\n","iter 1300 train loss = 2.9465739140654232\n","iter 1400 train loss = 2.945376689104187\n","iter 1500 train loss = 2.94511755240631\n","iter 1600 train loss = 2.9472670574129154\n","iter 1700 train loss = 2.947268462936999\n","iter 1800 train loss = 2.9462499765296157\n","iter 1900 train loss = 2.9461806948549283\n","iter 2000 train loss = 2.946581324002345\n","iter 2100 train loss = 2.946497272877883\n","iter 2200 train loss = 2.94695730992688\n","iter 2300 train loss = 2.9488274970948356\n","iter 2400 train loss = 2.9488004639038436\n","iter 2500 train loss = 2.9494443038429594\n","iter 2600 train loss = 2.9502132321879184\n","iter 2700 train loss = 2.9494082236358983\n","iter 2800 train loss = 2.9494426230533626\n","iter 2900 train loss = 2.948730229153652\n","iter 3000 train loss = 2.9480790234345733\n","iter 3100 train loss = 2.947743503397818\n","iter 3200 train loss = 2.9482933778200713\n","iter 3300 train loss = 2.9482287828856286\n","iter 3400 train loss = 2.948278888140083\n","iter 3500 train loss = 2.9487117885645486\n","iter 3600 train loss = 2.9480598172725156\n","iter 3700 train loss = 2.9487097431638043\n","iter 3800 train loss = 2.9487326028734815\n","iter 3900 train loss = 2.948330164174416\n","iter 4000 train loss = 2.948080694702096\n","iter 4100 train loss = 2.947985172049773\n","iter 4200 train loss = 2.947643908510891\n","iter 4300 train loss = 2.947660958848225\n","iter 4400 train loss = 2.9479565191387853\n","iter 4500 train loss = 2.9477947741522454\n","iter 4600 train loss = 2.9474762669505368\n","iter 4700 train loss = 2.9474542129846384\n","iter 4800 train loss = 2.947493038059407\n","iter 4900 train loss = 2.9471689168339705\n","Epoch 27 valid loss = 4.144053849496123\n","iter 0 train loss = 2.815944105459246\n","iter 100 train loss = 2.961823837662097\n","iter 200 train loss = 2.9428516710617445\n","iter 300 train loss = 2.9452438048323164\n","iter 400 train loss = 2.9429561643624553\n","iter 500 train loss = 2.942587634261802\n","iter 600 train loss = 2.9435628669252236\n","iter 700 train loss = 2.9448843978475954\n","iter 800 train loss = 2.944434708970299\n","iter 900 train loss = 2.946133703251653\n","iter 1000 train loss = 2.9472633599128417\n","iter 1100 train loss = 2.9459786401513046\n","iter 1200 train loss = 2.9472868623146407\n","iter 1300 train loss = 2.9473617950960063\n","iter 1400 train loss = 2.9486986044745724\n","iter 1500 train loss = 2.9487573741375783\n","iter 1600 train loss = 2.9507734321684786\n","iter 1700 train loss = 2.9494976473395256\n","iter 1800 train loss = 2.9503660127425055\n","iter 1900 train loss = 2.9491352868376253\n","iter 2000 train loss = 2.948993718731592\n","iter 2100 train loss = 2.9490542346714315\n","iter 2200 train loss = 2.94893560321243\n","iter 2300 train loss = 2.947908484122175\n","iter 2400 train loss = 2.9481993690785755\n","iter 2500 train loss = 2.9488270837758077\n","iter 2600 train loss = 2.949076175560848\n","iter 2700 train loss = 2.9486128644416727\n","iter 2800 train loss = 2.9474007339355506\n","iter 2900 train loss = 2.9474823506129275\n","iter 3000 train loss = 2.947652448442643\n","iter 3100 train loss = 2.9473905102670312\n","iter 3200 train loss = 2.94758872140109\n","iter 3300 train loss = 2.9476307067233676\n","iter 3400 train loss = 2.9472830734954143\n","iter 3500 train loss = 2.9473015739277804\n","iter 3600 train loss = 2.9477865330531214\n","iter 3700 train loss = 2.9473255851359457\n","iter 3800 train loss = 2.9472693559150787\n","iter 3900 train loss = 2.9472471506829234\n","iter 4000 train loss = 2.9471592246965628\n","iter 4100 train loss = 2.947691975201137\n","iter 4200 train loss = 2.947512585207436\n","iter 4300 train loss = 2.9471518327959734\n","iter 4400 train loss = 2.9468326658408204\n","iter 4500 train loss = 2.946540426806984\n","iter 4600 train loss = 2.9470857832209587\n","iter 4700 train loss = 2.94697091977251\n","iter 4800 train loss = 2.94652038803556\n","iter 4900 train loss = 2.946670496313446\n","Epoch 28 valid loss = 4.144279812143578\n","iter 0 train loss = 2.808224670569517\n","iter 100 train loss = 2.961483160330758\n","iter 200 train loss = 2.9506614134372233\n","iter 300 train loss = 2.9518338717690993\n","iter 400 train loss = 2.952041576364735\n","iter 500 train loss = 2.9515989547463892\n","iter 600 train loss = 2.9500256263464455\n","iter 700 train loss = 2.949155184839391\n","iter 800 train loss = 2.949227869495692\n","iter 900 train loss = 2.9473102515017096\n","iter 1000 train loss = 2.9478602273117103\n","iter 1100 train loss = 2.9488734710726217\n","iter 1200 train loss = 2.9478152808989986\n","iter 1300 train loss = 2.9477837024880444\n","iter 1400 train loss = 2.9474734796829365\n","iter 1500 train loss = 2.9477464551885415\n","iter 1600 train loss = 2.9480834911747187\n","iter 1700 train loss = 2.94790169346997\n","iter 1800 train loss = 2.948024304051129\n","iter 1900 train loss = 2.947992843177545\n","iter 2000 train loss = 2.948379129606062\n","iter 2100 train loss = 2.947880750283195\n","iter 2200 train loss = 2.9483215238400766\n","iter 2300 train loss = 2.948052971557647\n","iter 2400 train loss = 2.9475997745398166\n","iter 2500 train loss = 2.9479970321269384\n","iter 2600 train loss = 2.948526868376191\n","iter 2700 train loss = 2.9480201388260237\n","iter 2800 train loss = 2.9469746693001473\n","iter 2900 train loss = 2.947126655653705\n","iter 3000 train loss = 2.9473444094578483\n","iter 3100 train loss = 2.9475957708796097\n","iter 3200 train loss = 2.9475708363215536\n","iter 3300 train loss = 2.947548452195582\n","iter 3400 train loss = 2.9473508520366334\n","iter 3500 train loss = 2.9477454399565755\n","iter 3600 train loss = 2.948394533261169\n","iter 3700 train loss = 2.947940970880667\n","iter 3800 train loss = 2.9474152333327974\n","iter 3900 train loss = 2.9473202894560404\n","iter 4000 train loss = 2.947274293928552\n","iter 4100 train loss = 2.9475707616826954\n","iter 4200 train loss = 2.947788123927145\n","iter 4300 train loss = 2.947017585283307\n","iter 4400 train loss = 2.946638165067604\n","iter 4500 train loss = 2.946745754104094\n","iter 4600 train loss = 2.946991774210685\n","iter 4700 train loss = 2.946900926752089\n","iter 4800 train loss = 2.9465481073882307\n","iter 4900 train loss = 2.9467992984216824\n","Epoch 29 valid loss = 4.144298860107351\n","iter 0 train loss = 3.165653880049543\n","iter 100 train loss = 2.9093343317759097\n","iter 200 train loss = 2.919880813655343\n","iter 300 train loss = 2.9253537613232123\n","iter 400 train loss = 2.926813037168134\n","iter 500 train loss = 2.9325068339701974\n","iter 600 train loss = 2.935452240538635\n","iter 700 train loss = 2.93670805065113\n","iter 800 train loss = 2.9396172086141616\n","iter 900 train loss = 2.9395076591367157\n","iter 1000 train loss = 2.9382902118730545\n","iter 1100 train loss = 2.938298580594145\n","iter 1200 train loss = 2.9389400073129375\n","iter 1300 train loss = 2.9384637273285295\n","iter 1400 train loss = 2.937782403695296\n","iter 1500 train loss = 2.9390353602457395\n","iter 1600 train loss = 2.9386041628764703\n","iter 1700 train loss = 2.9388496729542455\n","iter 1800 train loss = 2.939006312894024\n","iter 1900 train loss = 2.9406308761659314\n","iter 2000 train loss = 2.9417534945264743\n","iter 2100 train loss = 2.942933307683416\n","iter 2200 train loss = 2.9433695670458335\n","iter 2300 train loss = 2.9436727284114372\n","iter 2400 train loss = 2.9442954094589036\n","iter 2500 train loss = 2.944277744770253\n","iter 2600 train loss = 2.9441627043825553\n","iter 2700 train loss = 2.9444453470072096\n","iter 2800 train loss = 2.944932670469174\n","iter 2900 train loss = 2.9454228958688677\n","iter 3000 train loss = 2.9451956375786024\n","iter 3100 train loss = 2.9454971880885608\n","iter 3200 train loss = 2.945258359489633\n","iter 3300 train loss = 2.945648485072867\n","iter 3400 train loss = 2.9461100656872325\n","iter 3500 train loss = 2.946580820349443\n","iter 3600 train loss = 2.946531431753701\n","iter 3700 train loss = 2.9458995404152066\n","iter 3800 train loss = 2.9463398087876365\n","iter 3900 train loss = 2.946063953265131\n","iter 4000 train loss = 2.946397834954965\n","iter 4100 train loss = 2.9456875916902003\n","iter 4200 train loss = 2.9459048601898363\n","iter 4300 train loss = 2.9460579794220223\n","iter 4400 train loss = 2.945808750779368\n","iter 4500 train loss = 2.945652572717055\n","iter 4600 train loss = 2.9453441111355074\n","iter 4700 train loss = 2.945653990878938\n","iter 4800 train loss = 2.945904505277767\n","iter 4900 train loss = 2.9462625442489965\n","Epoch 30 valid loss = 4.144334291924009\n","iter 0 train loss = 2.7487057042464116\n","iter 100 train loss = 2.9599130080871316\n","iter 200 train loss = 2.9517500394710603\n","iter 300 train loss = 2.960565673720156\n","iter 400 train loss = 2.954266156347935\n","iter 500 train loss = 2.9543012129645265\n","iter 600 train loss = 2.951336323019841\n","iter 700 train loss = 2.95086676251622\n","iter 800 train loss = 2.9489768096475246\n","iter 900 train loss = 2.951882732226168\n","iter 1000 train loss = 2.950096421052087\n","iter 1100 train loss = 2.9487626760736676\n","iter 1200 train loss = 2.9487219541313734\n","iter 1300 train loss = 2.9480883356585412\n","iter 1400 train loss = 2.948131871983806\n","iter 1500 train loss = 2.9476794028866737\n","iter 1600 train loss = 2.946526957240622\n","iter 1700 train loss = 2.946278199964373\n","iter 1800 train loss = 2.9461839811225095\n","iter 1900 train loss = 2.9470056840067276\n","iter 2000 train loss = 2.9466332008972076\n","iter 2100 train loss = 2.9456552857170712\n","iter 2200 train loss = 2.945848272867077\n","iter 2300 train loss = 2.9457110135548414\n","iter 2400 train loss = 2.946615255537745\n","iter 2500 train loss = 2.9463032421503685\n","iter 2600 train loss = 2.9467838263011714\n","iter 2700 train loss = 2.9468651628066787\n","iter 2800 train loss = 2.9465612024916195\n","iter 2900 train loss = 2.9468151888757608\n","iter 3000 train loss = 2.9470532648792487\n","iter 3100 train loss = 2.9470659986465026\n","iter 3200 train loss = 2.9472069480606535\n","iter 3300 train loss = 2.9472195427172805\n","iter 3400 train loss = 2.947491734247541\n","iter 3500 train loss = 2.947689177105095\n","iter 3600 train loss = 2.946543335870866\n","iter 3700 train loss = 2.946466569163977\n","iter 3800 train loss = 2.946727368575011\n","iter 3900 train loss = 2.9469368509803373\n","iter 4000 train loss = 2.946451217399621\n","iter 4100 train loss = 2.9464542467276877\n","iter 4200 train loss = 2.946567773590141\n","iter 4300 train loss = 2.9461315553729013\n","iter 4400 train loss = 2.945803863365572\n","iter 4500 train loss = 2.9461199347337694\n","iter 4600 train loss = 2.9460111571490994\n","iter 4700 train loss = 2.946164222340285\n","iter 4800 train loss = 2.945821646806154\n","iter 4900 train loss = 2.945794680374377\n","Epoch 31 valid loss = 4.144332660847788\n","iter 0 train loss = 3.048645159841954\n","iter 100 train loss = 2.9246511443374614\n","iter 200 train loss = 2.925931362564182\n","iter 300 train loss = 2.9387526725748385\n","iter 400 train loss = 2.945950041495187\n","iter 500 train loss = 2.942183415734142\n","iter 600 train loss = 2.9431937163382456\n","iter 700 train loss = 2.9441305910605284\n","iter 800 train loss = 2.9413001328308113\n","iter 900 train loss = 2.9438046868578405\n","iter 1000 train loss = 2.94665794445727\n","iter 1100 train loss = 2.948534733037362\n","iter 1200 train loss = 2.949404198423235\n","iter 1300 train loss = 2.9475568183753924\n","iter 1400 train loss = 2.94675488935585\n","iter 1500 train loss = 2.946063503824367\n","iter 1600 train loss = 2.9458277238771653\n","iter 1700 train loss = 2.946256748598762\n","iter 1800 train loss = 2.945204358667778\n","iter 1900 train loss = 2.9461222515670005\n","iter 2000 train loss = 2.9465439908817945\n","iter 2100 train loss = 2.9467877720282543\n","iter 2200 train loss = 2.9459820686434957\n","iter 2300 train loss = 2.9464410054575696\n","iter 2400 train loss = 2.9459356010852726\n","iter 2500 train loss = 2.9458273381148907\n","iter 2600 train loss = 2.9458882969772513\n","iter 2700 train loss = 2.9465631659686355\n","iter 2800 train loss = 2.947334370798542\n","iter 2900 train loss = 2.9478629914796564\n","iter 3000 train loss = 2.947231796607439\n","iter 3100 train loss = 2.9465832019327283\n","iter 3200 train loss = 2.946550773303678\n","iter 3300 train loss = 2.9465191030673075\n","iter 3400 train loss = 2.9465806858484447\n","iter 3500 train loss = 2.9457467946084948\n","iter 3600 train loss = 2.946313479453302\n","iter 3700 train loss = 2.94664072696639\n","iter 3800 train loss = 2.9468186841566055\n","iter 3900 train loss = 2.947352720402452\n","iter 4000 train loss = 2.9469227973068577\n","iter 4100 train loss = 2.946939628610621\n","iter 4200 train loss = 2.9469900929483854\n","iter 4300 train loss = 2.9467839105360047\n","iter 4400 train loss = 2.9465909528765435\n","iter 4500 train loss = 2.9466060596093784\n","iter 4600 train loss = 2.946547885890045\n","iter 4700 train loss = 2.9465768401185506\n","iter 4800 train loss = 2.9465184250084406\n","iter 4900 train loss = 2.946275283749751\n","Epoch 32 valid loss = 4.14435729921286\n","iter 0 train loss = 3.0057401238633084\n","iter 100 train loss = 2.9358392934330175\n","iter 200 train loss = 2.9377266621804914\n","iter 300 train loss = 2.928824634043783\n","iter 400 train loss = 2.938256121220808\n","iter 500 train loss = 2.9400881494234756\n","iter 600 train loss = 2.946518111737668\n","iter 700 train loss = 2.9494407217043626\n","iter 800 train loss = 2.9485569210257614\n","iter 900 train loss = 2.951462974795818\n","iter 1000 train loss = 2.9511729827696933\n","iter 1100 train loss = 2.95070404979619\n","iter 1200 train loss = 2.9493980994554048\n","iter 1300 train loss = 2.948421385156386\n","iter 1400 train loss = 2.948391966970304\n","iter 1500 train loss = 2.9492560475652043\n","iter 1600 train loss = 2.948729458667426\n","iter 1700 train loss = 2.9498555253739567\n","iter 1800 train loss = 2.9486802646477224\n","iter 1900 train loss = 2.9496612577162713\n","iter 2000 train loss = 2.947809137858812\n","iter 2100 train loss = 2.94856304241309\n","iter 2200 train loss = 2.949667389005078\n","iter 2300 train loss = 2.9486874051699545\n","iter 2400 train loss = 2.9488489977642582\n","iter 2500 train loss = 2.9484209303655153\n","iter 2600 train loss = 2.9474941865244735\n","iter 2700 train loss = 2.9480536872916376\n","iter 2800 train loss = 2.948074365595571\n","iter 2900 train loss = 2.947208197570553\n","iter 3000 train loss = 2.947775060063846\n","iter 3100 train loss = 2.9473083863937335\n","iter 3200 train loss = 2.9474489493278893\n","iter 3300 train loss = 2.947487538371016\n","iter 3400 train loss = 2.947725632304537\n","iter 3500 train loss = 2.94787752223896\n","iter 3600 train loss = 2.9476226386639413\n","iter 3700 train loss = 2.947210114211368\n","iter 3800 train loss = 2.9464488934282675\n","iter 3900 train loss = 2.946108066729017\n","iter 4000 train loss = 2.946382384521241\n","iter 4100 train loss = 2.9457166871515907\n","iter 4200 train loss = 2.945899937840472\n","iter 4300 train loss = 2.9460002628251853\n","iter 4400 train loss = 2.946307900852854\n","iter 4500 train loss = 2.946841784549697\n","iter 4600 train loss = 2.9468303545333696\n","iter 4700 train loss = 2.9466164440491545\n","iter 4800 train loss = 2.946845314259636\n","iter 4900 train loss = 2.946842008336795\n","Epoch 33 valid loss = 4.144414883423739\n","iter 0 train loss = 2.6263509897085338\n","iter 100 train loss = 2.943321621295103\n","iter 200 train loss = 2.941356307762893\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-055b57be1128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-66-60788f2c83b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, start_epoch, train_loader, valid_loader, plot_cache, best_val_loss)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"8QCbtOCkFzFz"},"source":["# **Loss Plots**"]},{"cell_type":"code","metadata":{"id":"FZkucGW2iyRP","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1606197745577,"user_tz":300,"elapsed":872,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"caf16dd7-5681-4f19-895a-20ab2aefdf23"},"source":["import numpy as np\n","epochs = np.array(list(range(len(plot_cache))))\n","plt.plot(epochs, [i[0] for i in plot_cache], label='Train loss')\n","plt.plot(epochs, [i[1] for i in plot_cache], label='Valid loss')\n","\n","plt.legend()\n","plt.title('Loss curves')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"kAK43ulYizPd","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1606197750110,"user_tz":300,"elapsed":809,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"3010b64b-e2fc-4722-880d-07f58aa4b5cc"},"source":["epochs = np.array(list(range(len(plot_cache))))\n","plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n","plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n","\n","plt.legend()\n","plt.title('PPL curves')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"bJIFtDM9-brw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606079632566,"user_tz":300,"elapsed":831,"user":{"displayName":"Amanda Kuznecov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWqCM-4eoQkiUBQFArDW57WqZyJdN1BVRKe01O=s64","userId":"10709662812019526205"}},"outputId":"d4bf7b25-ed68-4de0-ac06-add92667f81e"},"source":["plot_cache"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(4.178999205238422, 3.9995838571794624),\n"," (3.7387595745068576, 3.970521905980979),\n"," (3.609801376065514, 3.9797455094388416),\n"," (3.531458226792578, 3.9864388757263853),\n"," (3.4733490156119005, 4.00522057843881),\n"," (3.428383651885224, 4.021451378552952),\n"," (3.3891653735406995, 4.036160557662829),\n"," (3.3570368124613896, 4.056110930334154),\n"," (3.159931156325688, 3.961843560769118),\n"," (3.110519249888533, 3.9525630871400934),\n"," (3.087279403406589, 3.9474780927998565),\n"," (3.070284212858198, 3.9465592269579353),\n"," (3.054818345790841, 3.9434651952125996),\n"," (3.0431292958330034, 3.944003568139026),\n"," (3.031691180311305, 3.9439499231310946),\n"," (3.021517295702578, 3.9438028785914927),\n"," (3.012109515167703, 3.944246198420588),\n"," (3.002993845702707, 3.945858317226949),\n"," (2.994180524366184, 3.945712163091619),\n"," (2.965663178262328, 3.9423195937966833),\n"," (2.962404592400025, 3.941408846642568),\n"," (2.961548609877266, 3.941104610091348),\n"," (2.9596470695025787, 3.9410258642052582),\n"," (2.9586169866389023, 3.9408075896912007),\n"," (2.9575021587606556, 3.9407800469333307),\n"," (2.9556342219990515, 3.94077697682887),\n"," (2.9544113993689414, 3.940876498518727),\n"," (2.954270964638623, 3.940633780646715),\n"," (2.9533509965194362, 3.94068972224074),\n"," (2.9517367518022186, 3.940447036031492),\n"," (2.9489067665728705, 3.9403653715487468),\n"," (2.9483518765258845, 3.940363191848558),\n"," (2.948240924969064, 3.940363191848558)]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"jG-L5OqZzlCu"},"source":[""],"execution_count":null,"outputs":[]}]}